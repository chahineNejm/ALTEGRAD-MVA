{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n<h1>\n<h1>APM 53674: ALTeGraD</h1>\n<h2>Lab Session 1: Neural Machine Translation and Language Modeling</h2>\n<h4>Lecture: Prof. Michalis Vazirgiannis<br>\nLab: Dr. Hadi Abdine and Yang Zhang</h4>\n<h5>Monday, September 30, 2025</h5>\n<br>\n</center>\n\n<hr style=\"border:10px solid gray\"> </hr>\n<p style=\"text-align: justify;\">\nThis handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit <a href='https://forms.gle/9dyaes6dimfvyjwq6' target=\"_blank\">here</a> a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>October 05\n, 2025 11:59 PM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late → -5 pts; ]24, 48] hours late → -10 pts; > 48 hours late → not graded (zero).\n</p>\n<hr style=\"border:5px solid gray\"> </hr>\n","metadata":{"id":"PwXvz16rv753"}},{"cell_type":"markdown","source":"## <b>Neural Machine Translation</b></h2>","metadata":{"id":"002zIWSTw46F"}},{"cell_type":"markdown","source":"<h3><b>1. Learning Objective:</b></h2>\n<p style=\"text-align: justify;\">\nIn this lab, you will learn about sequence to sequence (seq2seq) architectures.\nMore precisely, we will implement the Neural Machine Translation (NMT) model described in <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a> using Python 3.6 and PyTorch (the latest version).\nThe only difference is that we will be using non-stacked RNNs, whereas <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a> uses stacked RNNs.\n\nWe will train our model on the task of English to French translation, using a set of sentence pairs from <a href='http://www.manythings.org/anki/'>http://www.manythings.org/anki/</a>, originally extracted from the Tatoeba project: <a href='https://tatoeba.org/eng/'>https://tatoeba.org/eng/</a>.\n\nOur dataset features 136,521 pairs for training and 34,130 pairs for testing, which is quite small, but enough for the purpose of this lab.\nThe average size of a source sentence is 7.6 while the average size of a target sentence is 8.3.\n\n$\\underline{\\textbf{Note}}$: the pairs have already been preprocessed.\nEach sentence was turned into a list of integers starting from 4.\nThe integers correspond to indexes in the source and target vocabularies, that have been constructed from the training set, and in which the most frequent words have index 4.\n0, 1, 2 and 3 are reserved respectively for the padding, out-of-vocabulary, start of sentence, and end of sentence special tokens.\n\n<h3><b>2. Recurrent Neural Networks:</b></h3>\n<p style=\"text-align: justify;\">\n\nWhile CNNs are good at dealing with grids, RNNs were specifically developed to be used with sequences.\nAs shown in Fig. 1, a RNN can be viewed as a chain of simple neural layers that share the same parameters.\nFrom a high level, a RNN is fed an ordered list of input vectors $\\big\\{x_{1},...,x_{T}\\big\\}$ as well as an initial hidden state $h_{0}$ initialized to all zeros, and returns an ordered list of hidden states $\\big\\{h_{1},...,h_{T}\\big\\}$, as well as an ordered list of output vectors $\\big\\{y_{1},...,y_{T}\\big\\}$.\nThe hidden states may serve as input to the RNN units above in the case of a stacked architecture, or directly be used as they are (e.g., by the attention mechanism).\nThe hidden states correspond more or less to the \"short-term\" memory of the network.\n<center>\n<table><tr>\n<td> <img src='https://1drv.ms/i/c/ae69638675180117/UQQXARh1hmNpIICuu4QBAAAAAMrZ9Edq70cBsjo?width=498&height=246' alt=\"Drawing\" width= '500px'/> </td>\n<td> <img src=\"https://1drv.ms/i/c/ae69638675180117/UQQXARh1hmNpIICuvIQBAAAAAJVN-1WBTh55QtY?width=1336&height=733\" alt=\"Drawing\" width='500px'/> </td>\n</tr></table>\n\n<b>Figure 1:</b> Left: 3 steps of an unrolled RNN (adapted from <a href='http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/'>Denny Britz' blog</a>). Right: 3 steps of an unrolled stacked RNN.\nThe hidden states at a given position flow vertically through the RNN layers. On both sides, each circle represents a  RNN unit. </a><br>\n</center>\n\n<h3><b>3. Sequence-to-sequence architecture:</b></h3>\n<p style=\"text-align: justify;\">\nOur input and output are sequences of words, respectively $x = \\big(x_1, \\dots ,x_{T_x}\\big)$ and $y = \\big(y_1, \\dots ,y_{T_y}\\big)$.\n$x$ and $y$ are usually referred to as the $\\textit{source}$ and $\\textit{target}$ sentences.\n<h4><b>3.1. Encoder</b></h4>\n<p style=\"text-align: justify;\">\nOur encoder is a non-stacked unidirectional RNN with GRU units (see the appendix for details about the GRU.)\n\n","metadata":{"id":"lCJvlnvsKALE"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils import data\nfrom torch.nn.utils.rnn import pad_sequence\nfrom tqdm import tqdm\nfrom nltk import word_tokenize\nimport sys\nimport json\nimport nltk\nnltk.download('punkt')\nnltk.download('punkt_tab')\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"id":"DB6pvLvlKbtD","outputId":"bae5359f-ae06-4815-cbf9-948fc2520d41","colab":{"base_uri":"https://localhost:8080/"},"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:36:33.871662Z","iopub.execute_input":"2025-10-04T14:36:33.872042Z","iopub.status.idle":"2025-10-04T14:36:43.411003Z","shell.execute_reply.started":"2025-10-04T14:36:33.872004Z","shell.execute_reply":"2025-10-04T14:36:43.410369Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"\n<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 1: </b><br>\nFill the gaps in the $\\texttt{forward}$ function of the $\\texttt{Encoder}$ class.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"LNLFXbJu7Kcp"}},{"cell_type":"code","source":"class Encoder(nn.Module):\n    '''\n    to be passed the entire source sequence at once\n    we use padding_idx in nn.Embedding so that the padding vector does not take gradient (always zero)\n    https://pytorch.org/docs/stable/nn.html#gru\n    '''\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n\n    def forward(self, input):\n        word_vectors = self.embedding(input)\n        hs,_ =  self.rnn(word_vectors)      # fill the gap # (seq,batch,feat)\n        return hs","metadata":{"id":"Kc8cQTFkKmif","outputId":"862aecd1-465f-490e-a36d-a9966767846f","colab":{"base_uri":"https://localhost:8080/","height":106},"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:36:43.412086Z","iopub.execute_input":"2025-10-04T14:36:43.412393Z","iopub.status.idle":"2025-10-04T14:36:43.417019Z","shell.execute_reply.started":"2025-10-04T14:36:43.412375Z","shell.execute_reply":"2025-10-04T14:36:43.416380Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"<h4><b>3.2. Decoder</b></h4>\n<p style=\"text-align: justify;\">\nOur decoder is a non-stacked unidirectional RNN.\nIt is a neural language model conditioned not only on the previously generated target words but also on the source sentence.\nMore precisely, it generates the target sentence $y=(y_1,\\dots,y_{T_y})$ one word $y_t$ at a time based on the distribution:\n\n\\begin{equation}\nP\\big[y_t|\\{y_{1},...,y_{t-1}\\},c_t\\big] = \\mathrm{softmax}\\big(W_s\\tilde{h}_t\\big)\n\\end{equation}\n\nwhere $\\tilde{h}_t$, the \\textit{attentional} hidden state, is computed as (biases are not shown for simplicity):\n\n\\begin{equation}\n\\tilde{h}_t = \\mathrm{tanh}\\big(W_c\\big[c_t;h_t\\big]\\big)\n\\end{equation}\n\n $h_t$ is the $t^{th}$ hidden state of the decoder, $c_t$ is the source context vector, and $\\big[;\\big]$ denotes concatenation. $W_s$ and $W_c$ are matrices of trainable parameters.\n\n$\\textbf{Note:}$ while all the inputs of the encoder (i.e., all the words of the input sentence) are known at encoding time, the decoder generates one target word at a time, and uses as input at time $t$ its prediction from time $t-1$.\n\n\n<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 2: </b><br>\nFill the gaps in the $\\texttt{forward}$ function of the $\\texttt{Decoder}$ class.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"gZgoCyV27q65"}},{"cell_type":"code","source":"class Decoder(nn.Module):\n    '''to be used one timestep at a time\n       see https://pytorch.org/docs/stable/nn.html#gru'''\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, padding_idx):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx)\n        self.rnn = nn.GRU(embedding_dim, hidden_dim)\n        self.ff_concat = nn.Linear(2*hidden_dim,hidden_dim)\n        self.predict = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, input, source_context, h):\n        word_vector = self.embedding(input) # fill the gap # (1,batch) -> (1,batch,feat)\n        _ ,h = self.rnn(word_vector) # fill the gap # (1,batch,feat)\n        tilde_h = torch.tanh(self.ff_concat(torch.cat([source_context,h],dim = 2)))# fill the gap # (1,batch,2*feat) -> (1,batch,feat)\n        prediction = self.predict(tilde_h) # (1,batch,feat) -> (1,batch,vocab)\n\n        return prediction, h\n\n","metadata":{"id":"h7tLaq4PK90q","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:36:43.417767Z","iopub.execute_input":"2025-10-04T14:36:43.417936Z","iopub.status.idle":"2025-10-04T14:36:43.468713Z","shell.execute_reply.started":"2025-10-04T14:36:43.417923Z","shell.execute_reply":"2025-10-04T14:36:43.468009Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"<h4><b>3.3. Global attention mechanism</b></h4>\n<p style=\"text-align: justify;\">\nThe context vector $c_t$ is computed as a weighted sum of the encoder's hidden states $\\bar{h}_i$.\nThe vector of weights $\\alpha_{t}$ is obtained by applying a softmax to the output of an $\\textit{alignment}$ operation ($\\texttt{score()}$) between the current target hidden state $h_t$ and all source hidden states $\\bar{h}_{i}$'s.\n$\\alpha_{t}$ indicates which words in the source sentence are the most likely to help in predicting the next word.\n$\\texttt{score()}$ can in theory be any comparison function.\nIn our implementation, we will use the $\\texttt{concat}$ attention formulation of <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a> (see section 3.1 of the paper).\nAn overview is provided in Fig. 2.  at a time, and uses as input at time $t$ its prediction from time $t-1$.\n<center>\n<img width='800px' src='https://1drv.ms/i/c/ae69638675180117/UQQXARh1hmNpIICutoQBAAAAAMg4GcKQUg3VOR8?width=1836&height=874' />\n<br>\n<b>Figure 2:</b>Summary of the $\\textit{global attention}$ mechanism <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a> <br>\n</center>\n\n\n<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 3: </b><br>\nFill the gaps in the $\\texttt{forward}$ function of the $\\texttt{seq2seqAtt}$ class.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"0mE7IkYx8Kjl"}},{"cell_type":"code","source":"#chahine\nclass seq2seqAtt(nn.Module):\n    '''\n    concat global attention a la Luong et al. 2015 (subsection 3.1)\n    https://arxiv.org/pdf/1508.04025.pdf\n    '''\n    def __init__(self, hidden_dim, hidden_dim_s, hidden_dim_t):\n        super(seq2seqAtt, self).__init__()\n        self.ff_concat = nn.Linear(hidden_dim_s+hidden_dim_t,hidden_dim)\n        self.ff_score = nn.Linear(hidden_dim,1,bias=False) # just a dot product here\n\n    def forward(self,target_h,source_hs):\n        target_h_rep = target_h.repeat(source_hs.size(0),1,1) # (1,batch,feat) -> (seq,batch,feat)\n        concat_output =self.ff_concat(torch.cat((source_hs,target_h_rep),dim = 2)) # fill the gap # source_hs is (seq,batch,feat)\n        scores = self.ff_score(torch.tanh(concat_output)) # (seq,batch,feat) -> (seq,batch,1)\n        scores = scores.squeeze(dim=2) # (seq,batch,1) -> (seq,batch). dim=2 because we don't want to squeeze the batch dim if batch size = 1\n        norm_scores = torch.softmax(scores, 0) # attention weights\n        source_hs_p = source_hs.permute((2,0,1)) # (seq,batch,feat) -> (feat,seq,batch)\n        weighted_source_hs = (norm_scores * source_hs_p) # (seq,batch) * (feat,seq,batch) (* checks from right to left that the dimensions match)\n        ct = torch.sum(weighted_source_hs.permute((1,2,0)),0,keepdim=True) # (feat,seq,batch) -> (seq,batch,feat) -> (1,batch,feat); keepdim otherwise sum squeezes\n        return ct, norm_scores.squeeze(1)","metadata":{"id":"JwUAUDL4KmoM","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:36:43.470359Z","iopub.execute_input":"2025-10-04T14:36:43.470787Z","iopub.status.idle":"2025-10-04T14:36:43.490331Z","shell.execute_reply.started":"2025-10-04T14:36:43.470770Z","shell.execute_reply":"2025-10-04T14:36:43.489652Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"<h3><b>4. Training and Evaluation:</b></h3>\n<p style=\"text-align: justify;\">\n\n<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 4: </b><br>\nFill the gaps in the $\\texttt{forward}$ function of the $\\texttt{seq2seqModel}$ class.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"nLurQq5qABAz"}},{"cell_type":"code","source":"class seq2seqModel(nn.Module):\n    '''the full seq2seq model'''\n    ARGS = ['vocab_s','source_language','vocab_t_inv','embedding_dim_s','embedding_dim_t',\n     'hidden_dim_s','hidden_dim_t','hidden_dim_att','do_att','padding_token',\n     'oov_token','sos_token','eos_token','max_size']\n    def __init__(self, vocab_s, source_language, vocab_t_inv, embedding_dim_s, embedding_dim_t,\n                 hidden_dim_s, hidden_dim_t, hidden_dim_att, do_att, padding_token,\n                 oov_token, sos_token, eos_token, max_size):\n        super(seq2seqModel, self).__init__()\n        self.vocab_s = vocab_s\n        self.source_language = source_language\n        self.vocab_t_inv = vocab_t_inv\n        self.embedding_dim_s = embedding_dim_s\n        self.embedding_dim_t = embedding_dim_t\n        self.hidden_dim_s = hidden_dim_s\n        self.hidden_dim_t = hidden_dim_t\n        self.hidden_dim_att = hidden_dim_att\n        self.do_att = do_att # should attention be used?\n        self.padding_token = padding_token\n        self.oov_token = oov_token\n        self.sos_token = sos_token\n        self.eos_token = eos_token\n        self.max_size = max_size\n\n        self.max_source_idx = max(list(vocab_s.values()))\n        print('max source index',self.max_source_idx)\n        print('source vocab size',len(vocab_s))\n\n        self.max_target_idx = max([int(elt) for elt in list(vocab_t_inv.keys())])\n        print('max target index',self.max_target_idx)\n        print('target vocab size',len(vocab_t_inv))\n\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n        self.encoder = Encoder(self.max_source_idx+1,self.embedding_dim_s,self.hidden_dim_s,self.padding_token).to(self.device)\n        self.decoder = Decoder(self.max_target_idx+1,self.embedding_dim_t,self.hidden_dim_t,self.padding_token).to(self.device)\n\n        if self.do_att:\n            self.att_mech = seq2seqAtt(self.hidden_dim_att,self.hidden_dim_s,self.hidden_dim_t).to(self.device)\n\n    def my_pad(self,my_list):\n        '''my_list is a list of tuples of the form [(tensor_s_1,tensor_t_1),...,(tensor_s_batch,tensor_t_batch)]\n        the <eos> token is appended to each sequence before padding\n        https://pytorch.org/docs/stable/nn.html#torch.nn.utils.rnn.pad_sequence'''\n        batch_source = pad_sequence([torch.cat((elt[0],torch.LongTensor([self.eos_token]))) for elt in my_list],batch_first=True,padding_value=self.padding_token)\n        batch_target = pad_sequence([torch.cat((elt[1],torch.LongTensor([self.eos_token]))) for elt in my_list],batch_first=True,padding_value=self.padding_token)\n        return batch_source,batch_target\n\n    def forward(self,input,max_size,is_prod):\n\n        if is_prod:\n            input = input.unsqueeze(1) # (seq) -> (seq,1) 1D input <=> we receive just one sentence as input (predict/production mode)\n\n        current_batch_size = input.size(1)\n\n        # fill the gap #\n        # use the encoder\n        source_hs = self.encoder(input)     # (seq,batch)/(seq,1) -> (seq,batch,feat)/(seq,1,feat)\n\n        # = = = decoder part (one timestep at a time)  = = =\n\n        target_h = torch.zeros(size=(1,current_batch_size,self.hidden_dim_t)).to(self.device) # init (1,batch,feat)\n\n        # fill the gap #\n        # (initialize target_input with the proper token)\n        \n        target_input = torch.LongTensor([self.sos_token]).repeat(current_batch_size).unsqueeze(0).to(self.device) # init (1,batch)\n\n        pos = 0\n        eos_counter = 0\n        logits = []\n        weights = []\n\n        while True:\n            #####\"\"\" chahine\n            if self.do_att:\n                source_context = self.att_mech(target_h,source_hs) # (1,batch,feat)\n                weights.append(source_context[1].tolist()) # attention weights\n                source_context = source_context[0]\n            else:\n                source_context = source_hs[-1,:,:].unsqueeze(0) # (1,batch,feat) last hidden state of encoder\n\n            # fill the gap #   chahine #########\"\"\n            # use the decoder\n            prediction, target_h = self.decoder(target_input,source_context,target_h)   # the decode takes in the input , source, h\n\n            logits.append(prediction) # (1,batch,vocab)\n\n            # fill the gap #\n            # get the next input to pass the decoder\n            \n            target_input = prediction.argmax(dim=-1)\n\n            eos_counter += torch.sum(target_input==self.eos_token).item()\n\n            pos += 1\n            if pos>=max_size or (eos_counter == current_batch_size and is_prod):\n                break\n\n        to_return = torch.cat(logits,0) # logits is a list of tensors -> (seq,batch,vocab)\n\n        if is_prod:\n            to_return = to_return.squeeze(dim=1) # (seq,vocab)\n\n        return to_return, weights\n\n    def fit(self, trainingDataset, testDataset, lr, batch_size, n_epochs, patience):\n\n        parameters = [p for p in self.parameters() if p.requires_grad]\n\n        optimizer = optim.Adam(parameters, lr=lr)\n\n        criterion = torch.nn.CrossEntropyLoss(ignore_index=self.padding_token) # the softmax is inside the loss!\n\n        # https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n        # we pass a collate function to perform padding on the fly, within each batch\n        # this is better than truncation/padding at the dataset level\n        train_loader = data.DataLoader(trainingDataset, batch_size=batch_size,\n                                       shuffle=True, collate_fn=self.my_pad) # returns (batch,seq)\n\n        test_loader = data.DataLoader(testDataset, batch_size=512,\n                                      collate_fn=self.my_pad)\n\n        tdqm_dict_keys = ['loss', 'test loss']\n        tdqm_dict = dict(zip(tdqm_dict_keys,[0.0,0.0]))\n\n        patience_counter = 1\n        patience_loss = 99999\n\n        for epoch in range(n_epochs):\n\n            with tqdm(total=len(train_loader),unit_scale=True,postfix={'loss':0.0,'test loss':0.0},\n                      desc=\"Epoch : %i/%i\" % (epoch, n_epochs-1),ncols=100) as pbar:\n                for loader_idx, loader in enumerate([train_loader, test_loader]):\n                    total_loss = 0\n                    # set model mode (https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n                    if loader_idx == 0:\n                        self.train()\n                    else:\n                        self.eval()\n                    for i, (batch_source,batch_target) in enumerate(loader):\n                        batch_source = batch_source.transpose(1,0).to(self.device) # RNN needs (seq,batch,feat) but loader returns (batch,seq)\n                        batch_target = batch_target.transpose(1,0).to(self.device) # (seq,batch)\n\n                        # are we using the model in production / as an API?\n                        is_prod = len(batch_source.shape)==1 # if False, 2D input (seq,batch), i.e., train or test\n\n                        if is_prod:\n                            max_size = self.max_size\n                            self.eval()\n                        else:\n                            max_size = batch_target.size(0) # no need to continue generating after we've exceeded the length of the longest ground truth sequence\n\n                        unnormalized_logits = self.forward(batch_source,max_size,is_prod)[0]\n\n                        sentence_loss = criterion(unnormalized_logits.flatten(end_dim=1),batch_target.flatten())\n\n                        total_loss += sentence_loss.item()\n\n                        tdqm_dict[tdqm_dict_keys[loader_idx]] = total_loss/(i+1)\n\n                        pbar.set_postfix(tdqm_dict)\n\n                        if loader_idx == 0:\n                            optimizer.zero_grad() # flush gradient attributes\n                            sentence_loss.backward() # compute gradients\n                            optimizer.step() # update\n                            pbar.update(1)\n\n            if total_loss > patience_loss:\n                patience_counter += 1\n            else:\n                patience_loss = total_loss\n                patience_counter = 1 # reset\n\n            if patience_counter>patience:\n                break\n\n    def sourceNl_to_ints(self,source_nl):\n        '''converts natural language source sentence into source integers'''\n        source_nl_clean = source_nl.lower().replace(\"'\",' ').replace('-',' ')\n        source_nl_clean_tok = word_tokenize(source_nl_clean,self.source_language)\n        source_ints = [int(self.vocab_s[elt]) if elt in self.vocab_s else \\\n                       self.oov_token for elt in source_nl_clean_tok]\n\n        source_ints = torch.LongTensor(source_ints).to(self.device)\n        return source_ints\n\n    def targetInts_to_nl(self,target_ints):\n        '''converts integer target sentence into target natural language'''\n        return ['<PAD>' if elt==self.padding_token else '<OOV>' if elt==self.oov_token \\\n                else '<EOS>' if elt==self.eos_token else '<SOS>' if elt==self.sos_token\\\n                else self.vocab_t_inv[elt] for elt in target_ints]\n\n    def predict(self,source_nl):\n        source_ints = self.sourceNl_to_ints(source_nl)\n        logits = self.forward(source_ints,self.max_size,True) # (seq) -> (<=max_size,vocab)\n        target_ints = logits[0].argmax(-1).squeeze() # (<=max_size,1) -> (<=max_size)\n        target_nl = self.targetInts_to_nl(target_ints.tolist())\n        return ' '.join(target_nl), logits[1]\n\n    def save(self,path_to_file):\n        attrs = {attr:getattr(self,attr) for attr in self.ARGS}\n        attrs['state_dict'] = self.state_dict()\n        torch.save(attrs,path_to_file)\n\n    @classmethod # a class method does not see the inside of the class (a static method does not take self as first argument)\n    def load(cls,path_to_file):\n        attrs = torch.load(path_to_file, map_location=lambda storage, loc: storage) # allows loading on CPU a model trained on GPU, see https://discuss.pytorch.org/t/on-a-cpu-device-how-to-load-checkpoint-saved-on-gpu-device/349/6\n        state_dict = attrs.pop('state_dict')\n        new = cls(**attrs) # * list and ** names (dict) see args and kwargs\n        new.load_state_dict(state_dict)\n        return new","metadata":{"id":"FYX0K3dNK-c9","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:36:43.491149Z","iopub.execute_input":"2025-10-04T14:36:43.491440Z","iopub.status.idle":"2025-10-04T14:36:43.521964Z","shell.execute_reply.started":"2025-10-04T14:36:43.491424Z","shell.execute_reply":"2025-10-04T14:36:43.521440Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Download the data and the pretrained model:","metadata":{"id":"md-HH53wAq3S"}},{"cell_type":"code","source":"import urllib\n\nurllib.request.urlretrieve(\"https://nuage.lix.polytechnique.fr/public.php/dav/files/FLttXTmeFSDNx7H\", \"data.zip\")\nurllib.request.urlretrieve(\"https://nuage.lix.polytechnique.fr/public.php/dav/files/6btZHdtYnyAAH3x\", \"pretrained_moodle.pt\")\n!unzip data.zip\n\npath_to_data = './data/'\npath_to_save_models = './'","metadata":{"id":"datl5SFtJ9Br","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:36:43.522815Z","iopub.execute_input":"2025-10-04T14:36:43.523064Z","iopub.status.idle":"2025-10-04T14:36:46.830833Z","shell.execute_reply.started":"2025-10-04T14:36:43.523041Z","shell.execute_reply":"2025-10-04T14:36:46.830086Z"}},"outputs":[{"name":"stdout","text":"Archive:  data.zip\n   creating: data/\n  inflating: __MACOSX/._data         \n  inflating: data/pairs_train_ints.txt  \n  inflating: __MACOSX/data/._pairs_train_ints.txt  \n  inflating: data/pairs_test_ints.txt  \n  inflating: __MACOSX/data/._pairs_test_ints.txt  \n  inflating: data/README.txt         \n  inflating: __MACOSX/data/._README.txt  \n  inflating: data/vocab_target.json  \n  inflating: __MACOSX/data/._vocab_target.json  \n  inflating: data/vocab_source.json  \n  inflating: __MACOSX/data/._vocab_source.json  \n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"### Define the dataloader:","metadata":{"id":"AmBgproQAv8_"}},{"cell_type":"code","source":"class Dataset(data.Dataset):\n  def __init__(self, pairs):\n        self.pairs = pairs\n\n  def __len__(self):\n        return len(self.pairs) # total nb of observations\n\n  def __getitem__(self, idx):\n        source, target = self.pairs[idx] # one observation\n        return torch.LongTensor(source), torch.LongTensor(target)\n\ndef load_pairs(train_or_test):\n    with open(path_to_data + 'pairs_' + train_or_test + '_ints.txt', 'r', encoding='utf-8') as file:\n        pairs_tmp = file.read().splitlines()\n    pairs_tmp = [elt.split('\\t') for elt in pairs_tmp]\n    pairs_tmp = [[[int(eltt) for eltt in elt[0].split()],[int(eltt) for eltt in \\\n                  elt[1].split()]] for elt in pairs_tmp]\n    return pairs_tmp","metadata":{"id":"wZCiFl61LPQj","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:36:46.831774Z","iopub.execute_input":"2025-10-04T14:36:46.832048Z","iopub.status.idle":"2025-10-04T14:36:46.837910Z","shell.execute_reply.started":"2025-10-04T14:36:46.832010Z","shell.execute_reply":"2025-10-04T14:36:46.837269Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 5: </b><br>\nCheck that your implementation is correct by running $\\texttt{the following cell}$ for a few epochs and verifying that the loss decreases.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"a1yDxMlvA1yH"}},{"cell_type":"code","source":"do_att = True # should always be set to True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\npairs_train = load_pairs('train')\npairs_test = load_pairs('test')\n\nwith open(path_to_data + 'vocab_source.json','r') as file:\n    vocab_source = json.load(file) # word -> index\n\nwith open(path_to_data + 'vocab_target.json','r') as file:\n    vocab_target = json.load(file) # word -> index\n\nvocab_target_inv = {v:k for k,v in vocab_target.items()} # index -> word\nprint('data loaded')\ntraining_set = Dataset(pairs_train)\ntest_set = Dataset(pairs_test)\nprint('data prepared')\nprint('= = = attention-based model?:',str(do_att),'= = =')\n\nmodel = seq2seqModel(vocab_s=vocab_source,\n                      source_language='english',\n                      vocab_t_inv=vocab_target_inv,\n                      embedding_dim_s=40,\n                      embedding_dim_t=40,\n                      hidden_dim_s=30,\n                      hidden_dim_t=30,\n                      hidden_dim_att=20,\n                      do_att=do_att,\n                      padding_token=0,\n                      oov_token=1,\n                      sos_token=2,\n                      eos_token=3,\n                      max_size=30).to(device) # max size of generated sentence in prediction mode\n\nmodel.fit(training_set,test_set,lr=0.001,batch_size=64,n_epochs=20,patience=2)\nmodel.save(path_to_save_models + 'my_model.pt')\n","metadata":{"id":"kSZ-cvSuLQVt","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T14:36:46.838823Z","iopub.execute_input":"2025-10-04T14:36:46.839036Z","iopub.status.idle":"2025-10-04T15:10:55.113300Z","shell.execute_reply.started":"2025-10-04T14:36:46.839020Z","shell.execute_reply":"2025-10-04T15:10:55.112663Z"}},"outputs":[{"name":"stdout","text":"data loaded\ndata prepared\n= = = attention-based model?: True = = =\nmax source index 5281\nsource vocab size 5278\nmax target index 7459\ntarget vocab size 7456\n","output_type":"stream"},{"name":"stderr","text":"Epoch : 0/19: 100%|█████████████████| 2.13k/2.13k [01:43<00:00, 20.6it/s, loss=5.29, test loss=4.74]\nEpoch : 1/19: 100%|█████████████████| 2.13k/2.13k [01:42<00:00, 20.8it/s, loss=4.53, test loss=4.32]\nEpoch : 2/19: 100%|█████████████████| 2.13k/2.13k [01:42<00:00, 20.7it/s, loss=4.22, test loss=4.09]\nEpoch : 3/19: 100%|█████████████████| 2.13k/2.13k [01:42<00:00, 20.8it/s, loss=4.02, test loss=3.93]\nEpoch : 4/19: 100%|█████████████████| 2.13k/2.13k [01:41<00:00, 20.9it/s, loss=3.87, test loss=3.82]\nEpoch : 5/19: 100%|█████████████████| 2.13k/2.13k [01:42<00:00, 20.9it/s, loss=3.76, test loss=3.72]\nEpoch : 6/19: 100%|█████████████████| 2.13k/2.13k [01:42<00:00, 20.8it/s, loss=3.66, test loss=3.64]\nEpoch : 7/19: 100%|█████████████████| 2.13k/2.13k [01:41<00:00, 21.0it/s, loss=3.58, test loss=3.57]\nEpoch : 8/19: 100%|█████████████████| 2.13k/2.13k [01:42<00:00, 20.9it/s, loss=3.51, test loss=3.51]\nEpoch : 9/19: 100%|█████████████████| 2.13k/2.13k [01:42<00:00, 20.8it/s, loss=3.44, test loss=3.46]\nEpoch : 10/19: 100%|████████████████| 2.13k/2.13k [01:41<00:00, 21.0it/s, loss=3.38, test loss=3.41]\nEpoch : 11/19: 100%|████████████████| 2.13k/2.13k [01:42<00:00, 20.8it/s, loss=3.33, test loss=3.37]\nEpoch : 12/19: 100%|████████████████| 2.13k/2.13k [01:41<00:00, 20.9it/s, loss=3.29, test loss=3.34]\nEpoch : 13/19: 100%|████████████████| 2.13k/2.13k [01:42<00:00, 20.8it/s, loss=3.25, test loss=3.32]\nEpoch : 14/19: 100%|████████████████| 2.13k/2.13k [01:42<00:00, 20.9it/s, loss=3.21, test loss=3.27]\nEpoch : 15/19: 100%|████████████████| 2.13k/2.13k [01:41<00:00, 20.9it/s, loss=3.17, test loss=3.25]\nEpoch : 16/19: 100%|████████████████| 2.13k/2.13k [01:41<00:00, 21.0it/s, loss=3.14, test loss=3.22]\nEpoch : 17/19: 100%|████████████████| 2.13k/2.13k [01:41<00:00, 21.1it/s, loss=3.12, test loss=3.21]\nEpoch : 18/19: 100%|████████████████| 2.13k/2.13k [01:41<00:00, 21.1it/s, loss=3.09, test loss=3.19]\nEpoch : 19/19: 100%|████████████████| 2.13k/2.13k [01:41<00:00, 21.1it/s, loss=3.07, test loss=3.17]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 6: </b><br>\nRun the following cell using the pre-trained weights.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"WjzknQ0EBTIF"}},{"cell_type":"code","source":"model = seq2seqModel.load(path_to_save_models + 'pretrained_moodle.pt')\nto_test = ['I am a student.',\n            'I have a red car.',  # inversion captured\n            'I love playing video games.',\n            'This river is full of fish.', # plein vs pleine (accord)\n            'The fridge is full of food.',\n            'The cat fell asleep on the mat.',\n            'my brother likes pizza.', # pizza is translated to 'la pizza'\n            'I did not mean to hurt you', # translation of mean in context\n            'She is so mean',\n            'Help me pick out a tie to go with this suit!', # right translation\n            \"I can't help but smoking weed\", # this one and below: hallucination\n            'The kids were playing hide and seek',\n            'The cat fell asleep in front of the fireplace']\n\nfor elt in to_test:\n    print('= = = = = \\n','%s -> %s' % (elt, model.predict(elt)[0]))","metadata":{"id":"FwUInZMyQzci","trusted":true,"execution":{"iopub.status.busy":"2025-10-04T15:10:55.114144Z","iopub.execute_input":"2025-10-04T15:10:55.114493Z","iopub.status.idle":"2025-10-04T15:10:55.597731Z","shell.execute_reply.started":"2025-10-04T15:10:55.114474Z","shell.execute_reply":"2025-10-04T15:10:55.597035Z"}},"outputs":[{"name":"stdout","text":"max source index 5281\nsource vocab size 5278\nmax target index 7459\ntarget vocab size 7456\n= = = = = \n I am a student. -> un étudiant un un un un un un un un un un un un un un un un un un un un un un un un un un un un\n= = = = = \n I have a red car. -> un un un un un un un un un un un un un un un un un un un un un un un un un un un un un un\n= = = = = \n I love playing video games. -> le la la la la la la la la la la la la la la la la la la la la la la la la la la la la la\n= = = = = \n This river is full of fish. -> l pleine de ce une entre plein de une entre plein de une entre plein de une entre plein de une entre plein de une entre plein de une entre\n= = = = = \n The fridge is full of food. -> l <OOV> une quelle en une quelle en une quelle en une quelle en une quelle en une quelle en une quelle en une quelle en une quelle en une\n= = = = = \n The cat fell asleep on the mat. -> le le sur ce le le sur ce le le sur ce le le sur ce le le sur ce le le sur ce le le sur ce le le\n= = = = = \n my brother likes pizza. -> mon la la la la la la la la la la la la la la la la la la la la la la la la la la la la la\n= = = = = \n I did not mean to hurt you -> je te je te je te je te je te je te je te je te je te je te je te je te je te je te je te\n= = = = = \n She is so mean -> si si si si si si si si si si si si si si si si si si si si si si si si si si si si si si\n= = = = = \n Help me pick out a tie to go with this suit! -> quelqu avec vous vous avec avec vous vous avec avec vous vous avec avec vous vous avec avec vous vous avec avec vous vous avec avec vous vous avec avec\n= = = = = \n I can't help but smoking weed -> quiconque ne peut il je m a fumer m a fumer m a fumer m a fumer m a fumer m a fumer m a fumer m a fumer m\n= = = = = \n The kids were playing hide and seek -> le cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache cache\n= = = = = \n The cat fell asleep in front of the fireplace -> le devant le en le le le le le le le le le le le le le le le le le le le le le le le le le le\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"<h3><b>5. Questions:</b></h3>\n\n<b><h4><font color='red'>\n<hr style=\"border:10px solid red\"> </hr>\nQuestion 1 (5 points): </b><br>\nWhat do you think about our greedy decoding strategy? Base your answer on slides 87-95 from this <a href='https://nlp.stanford.edu/projects/nmt/Luong-Cho-Manning-NMT-ACL2016-v4.pdf'>presentation</a> (taken from this <a href='https://sites.google.com/site/acl16nmt/home'>ACL tutorial</a>).\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>\n\n\n\n\n\n\n","metadata":{"id":"-4I1ROQnCIcZ"}},{"cell_type":"markdown","source":"<b><h4><font color='green'>\n<hr style=\"border:10px solid red\"> </hr>\nAnswer 1: </b><br>\nStating from the do not even think of trying it out , I would say that the definit answer is \"there is better\". What we did in this TP is called an exhaustive search and is the lowest decoding basis. it relies on extracting the transaltion with the best score after assessing all possibilities. this is realluy slow compared to sampling methods or gready/beam heuristic . this methods have the added benefit of either having a faster conversion with relative ...\n\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>\n\n","metadata":{"id":"t8j_ppwvJnWk"}},{"cell_type":"markdown","source":"<b><h4><font color='red'>\n<hr style=\"border:10px solid red\"> </hr>\nQuestion 2 (5 points): </b><br>\nWhat major problem do you observe with our translations?\nHow could we remediate this issue? You may find inspiration from reading <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a> and <a href='https://arxiv.org/abs/1601.04811'>[Tu et al., 2016]</a>.\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"z0m_Hw-NJopL"}},{"cell_type":"markdown","source":"<b><h4><font color='green'>\n<hr style=\"border:10px solid red\"> </hr>\nAnswer 2: </b><br>\n\nGiven our use cases, we can distinguish two primary issues with our seq2seq model. The first is the obvious one: it gets stuck in repetition loops. The second is more subtle in our examples: the model skips words (e.g., The kids were playing hide and seek → le cache-cache), effectively “picking and choosing” what to translate. As noted in Tu et al., 2016, these are over-translation (repetition) and under-translation (omission). A standard fix is coverage: maintain a small data structurethat tracks how much each source position has already been attended to, and penalize re-attending or neglecting tokens. In practice, coverage reduces both repetition and skipping.\n\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>\n\n","metadata":{"id":"eExiju09Jhlp"}},{"cell_type":"markdown","source":"<b><h4><font color='red'>\n<hr style=\"border:10px solid red\"> </hr>\nQuestion 3 (5 points): </b><br>\nWrite some code to visualize source/target alignments in the style of Fig. 3 in <a href='https://arxiv.org/abs/1409.0473'>[Bahdanau et al., 2014]</a> or Fig. 7 in <a href='https://arxiv.org/abs/1508.04025'>[Luong et al., 2015]</a>.\nInterpret your figures for some relevant examples (e.g. to illustrate adjective-noun inversion)\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"RawMHJkMJk-r"}},{"cell_type":"code","source":"src_text = \"red bus\"\nx_pred, y_attn = model.predict(src_text)   # x_pred: predicted tokens, y_attn: attentions ~ [tgt_len, src_len]\ny_attn = y_attn[:4]\n\ntry:\n    import torch\n    heat = torch.stack([t.detach().cpu() for t in y_attn]).squeeze().numpy()\nexcept Exception:\n    heat = np.asarray(y_attn, dtype=float)\n\nsrc = src_text.split()\ntgt = list(x_pred) if isinstance(x_pred, (list, tuple)) else str(x_pred).split()\n\nT_src = len(src)\nT_tgt = min(len(tgt), heat.shape[0])\nheat = heat[:T_tgt, :T_src]\n\n# plot: x = PREDICTED target tokens, y = source tokens\nfig, ax = plt.subplots(figsize=(8,5))\nax.imshow(heat.T, cmap=\"gray\", aspect=\"auto\")\nax.xaxis.tick_top()\nax.set_xticks(range(T_tgt)); ax.set_xticklabels(tgt[:T_tgt], rotation=45, ha='left')\nax.set_yticks(range(T_src)); ax.set_yticklabels(src)\nplt.tight_layout()\nplt.savefig(\"attn_pred_only.png\", dpi=200, bbox_inches=\"tight\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T15:22:29.075453Z","iopub.execute_input":"2025-10-04T15:22:29.075742Z","iopub.status.idle":"2025-10-04T15:22:29.509195Z","shell.execute_reply.started":"2025-10-04T15:22:29.075719Z","shell.execute_reply":"2025-10-04T15:22:29.508355Z"},"jupyter":{"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":"src_text = \"can not\"\nx_pred, y_attn = model.predict(src_text)   # x_pred: predicted tokens, y_attn: attentions ~ [tgt_len, src_len]\ny_attn = y_attn[:3]\n\ntry:\n    import torch\n    heat = torch.stack([t.detach().cpu() for t in y_attn]).squeeze().numpy()\nexcept Exception:\n    heat = np.asarray(y_attn, dtype=float)\n\nsrc = src_text.split()\ntgt = list(x_pred) if isinstance(x_pred, (list, tuple)) else str(x_pred).split()\n\nT_src = len(src)\nT_tgt = min(len(tgt), heat.shape[0])\nheat = heat[:T_tgt, :T_src]\n\n# plot: x = PREDICTED target tokens, y = source tokens\nfig, ax = plt.subplots(figsize=(8,5))\nax.imshow(heat.T, cmap=\"gray\", aspect=\"auto\")\nax.xaxis.tick_top()\nax.set_xticks(range(T_tgt)); ax.set_xticklabels(tgt[:T_tgt], rotation=45, ha='left')\nax.set_yticks(range(T_src)); ax.set_yticklabels(src)\nplt.tight_layout()\nplt.savefig(\"attn_pred_only.png\", dpi=200, bbox_inches=\"tight\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T15:31:25.375274Z","iopub.execute_input":"2025-10-04T15:31:25.376128Z","iopub.status.idle":"2025-10-04T15:31:25.799876Z","shell.execute_reply.started":"2025-10-04T15:31:25.376100Z","shell.execute_reply":"2025-10-04T15:31:25.799211Z"},"jupyter":{"source_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 800x500 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAxYAAAHpCAYAAAAf5apCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAcnUlEQVR4nO3de5DV9X3/8dcudxUIWpU4QqiJnQaNgaBtEknMajLGGzRxgu2QJsZcWqc2DuqADiJQjWjaSUw0WiVFoqmoVOSSpG2sTRxSL4kCJkHsRFABXSQpBhatCOz+/nCyv2DUSt7g2V0fj5kdOXvO2fNmnC9znufz/Xy3qaOjoyMAAAAFzY0eAAAA6P6EBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgA0GO1t7e/6n0dHR1v4CQAPV/vRg8AAHtDe3t7mptf+vzsjjvuyMaNG7Nly5ZMmDAhw4cPT69evRo8IUDP0tThIxsAerDJkyfn29/+dsaOHZtHHnkkvXr1yrnnnpvPfOYzaWpqavR4AD2GFQsAeqxbbrklt9xyS773ve9l1KhRufPOO3P66afnoIMOEhUAe5g9FgD0WGvXrs3xxx+fUaNGZd68eTnzzDPzjW98I6eeemqef/75rFu3rtEjAvQYwgKAHuGVNmqvW7cuQ4YMybJly/KFL3whV1xxRc4+++x0dHTkW9/6VubPn5/t27c3YFqAnkdYANDt/fZG7fvvvz8bNmxIkkyYMCFz5szJ0Ucfneuvvz5nn312kuSFF17I4sWLs27duvTp06dhcwP0JMICgG7tt6Ni6tSpmTRpUv793/89L7zwQt797nfn3HPPzdChQ9PW1pZNmzZlxYoVOf3007Nhw4b8/d//fYOnB+g5XBUKgB7h4osvzvXXX5/bbrstY8aMyeDBg5MkTz31VK6++upcffXVGThwYA4++OAceOCB+dd//df06dMnO3fudOlZgD1AWADQ7f385z/PGWeckeuuuy4f/OAHs2nTpqxfvz7f//7309LSkjFjxmT16tVZs2ZNDj744Bx55JFpbm7Ojh070ru3CyQC7An+NQWg2+no6NjlcrGDBg3Ktm3bsn79+ixfvjzXXXddfvSjHyV56fdY3HXXXTnhhBPy9re/vfM57e3togJgD7LHAoBupb29vTMqVqxYkQ0bNmTw4MF573vfm8svvzzvfe9707dv31x++eV56KGH8v73vz/33Xff7/yc3+zLAGDP8FENAN3GyzdqL126NOecc04mTJiQmTNnprW1Nb169cr73ve+JMmOHTuyY8eOHHjggY0cG+BNwR4LALqdiy++ODfccENuuumm/Omf/mmGDBmyy/3PPfdcnnrqqUyaNCmtra358Y9/7LQngL3MOjAAXdoDDzywy+0VK1bk9ttvzx133JGPfvSjaW5uzqpVqzJ79uz813/9V5Lk5ptvznnnnZe2trY88MAD6d27d3bu3NmI8YH/w913353ly5c3egz2AB/fANBlzZo1K0uWLMm9997b+b399tsvffv2zaZNm/Lggw/mn/7pn/KDH/wgSfLEE0/kBz/4QU4++eQcfPDBGTduXHr16uXqT9BFXXjhhVm8eHEuvPDCjBgx4ndWH+lenAoFQJf2myhYs2ZNDjvssKxfvz5nnXVWnn322Tz88MP5/Oc/n4985CP5kz/5k3z84x/PxIkT87d/+7edz/d7KqBruvTSS3PNNdfk9ttvz/ve97707dt3l/tffvU3uj4f3wDQpfXu3TtLlizJ+PHjs2TJkpxyyim57rrr8thjj2WfffbJ2LFj09TUlPb29rS3t6d///67PF9UQNfz9NNPZ/Hixfna176W4447Lq2trVm9enUWL16coUOH5rzzzhMV3ZCwAKDL+e2rPyXJaaedlk9+8pOZOHFi5s2bl5NOOqnzd1L87//+bzZu3Jizzz477e3t+cxnPtOosYHXad99902/fv3yyCOP5Hvf+15uvvnmrFmzJr17984jjzySTZs25bLLLmv0mOwmm7fpsZzlB93Xb6JiwYIFWbFiRZLkpptuysc+9rFMmDAh//Zv/5bkpQC5/vrr84UvfCFtbW257777bNSGbmDffffNu9/97vzHf/xHxo0bl2HDhuWKK67I0qVLc9ppp+WFF15o9Ij8HqxY0COsWrUqmzdvztatWzN27Nj079/fEip0Yx0dHXnmmWcyYcKEjBs3LpdeemmOOOKI3HjjjUmSCRMmZP78+TnxxBNz6qmn5g/+4A/yF3/xFzZqQxf24IMPJkmampoyZsyYXHXVVVmzZk127NiRI444ovNxTz75ZIYPH96oMSmweZtub8GCBTn33HNz6KGH5r//+7/zgQ98IJ/+9Kfz8Y9/vNGjAbvhlTZq/uQnP8mf/dmf5f3vf3+mT5+eI488Mkly1llnZcGCBbnxxhvzsY99rPPxNmpD1zRt2rTMmzcvzc3NeeaZZzJt2rScd955nauTW7duzRNPPJELLrggra2teeihh3xA0A05FYpu7d57783nPve5TJ8+Pffdd1/uuOOOLFmyJL/61a8aPRqwm34TFc8991ySl0LjmGOOyaJFi7J06dLMmDEjK1euTJLMmTMnLS0tufbaa3f5GaICup7LLrsss2fPzpw5c7JixYqceeaZmTx5cmbMmJEXX3wxSXLnnXdm8uTJ6ejoyIMPPuiUxm7KigXd2lVXXZV77rknd955Z37xi1/k5JNPTktLS2644YYkyaZNm7L//vs3eErg9Zo1a1ZWrlyZf/iHf8jQoUM7VzEeeuihtLS05MQTT8wll1ySd73rXUl+d5M30LU8+uijueCCC3L22WfnlFNOyaJFi3LmmWdm/Pjx+fa3v52pU6dm5syZ2bFjR5YuXZrjjjsuzc3NTmnspvxrTLf29NNPZ8SIEUmSlpaWHH/88bn++uuTJPPnz8/ChQuzY8eOBk4IvJb29vZdbo8cOTK33HJLZs6cmQ0bNnReRnbMmDG5/PLLs2jRolx00UVZvXp1kpc2eb/8ZwBdxwEHHJDTTjstH/rQh7J06dL8zd/8TS677LLMnTs3n/rUp3LppZfmi1/8Ynr37p2WlpbOY1pUdE/Cgm5n06ZNef7555O8FBPf/OY3M2jQoHziE5/Idddd13k6xfe///0sXbo027dvb+S4wKv47dWGxx57LOvWrcv48ePzwAMPZPbs2bnkkkvS2tra+Zh+/fpl3LhxaW5uzh/+4R92/hwrFtD1PPbYY3nqqacyaNCg/NVf/VX23Xff3H777WlpaclnP/vZJMlBBx2UlpaWPPzww7t8QOCY7r78n6NbWbhwYcaNG5dRo0Zl+vTp6devX84555wMGDAgJ510Upqbm/Pss89m6tSpWbx4caZMmZIBAwY0emzgZTo6OjrfPFx44YU57bTTMnr06HzgAx/IL3/5yyxbtixz5szJjBkzcv/992fr1q357ne/m0984hNZvHixlQrown5zTI8aNSonnnhivvGNbyRJVq5cmY6OjvTv3z/bt2/Po48+mvPPPz/33HNPmpubXSa+B7DHgm5j2bJlOf7443P++efnf/7nf/KjH/0o73jHOzJmzJg88cQTmT17dkaOHJn+/funtbU1CxcuzOjRoxs9NvAyv71Sceutt2bSpEn5x3/8x/z617/Oz3/+83zlK1/JzTffnKOOOiqnnHJK2tvb06tXrwwePDgPPvhg+vTp84pXkAIa7+XH9MqVK/PVr3411157bYYPH56TTjopp556ap588sl0dHRk2bJl6d27t2O6hxAWdAurV6/OvHnz0tTUlKlTpyZJlixZkquvvjpDhgzJxIkTc8ABB2Tp0qV529velmOPPdY1sKGL++EPf5h//ud/zsiRIzNp0qQkSVtbW2688cZMmTIl//mf/5lDDjkky5YtS1tbWyZOnOj3VEAX9mrH9Ny5c3PhhRdmzpw56dWrVxYuXJiDDz44V155ZefVn1zRrWcQFnR5W7ZsyQknnJC1a9fmrLPOyqxZszrvW7x4ca666qoMGTIkU6dOzXve854GTgq8Xhs2bMjYsWOzcePGTJkypfMDgyR59tlnc+aZZ2b48OG5+uqrd3meNyDQNb3WMb1p06Z89rOfzbBhw/L1r389L774Yvr27ZskPijoYeyxoMsbNGhQbrjhhrzlLW/J0qVLO69jnyTjxo3LBRdckDVr1uQrX/lKnn/+eedoQjcwdOjQLFiwIAcddFAWLFiQ5cuXd943ZMiQHHjggZ1XfvptogK6ptc6pvfff/8ccMAB+cUvfpEknVGRRFT0MMKCbmH06NGZP39+nnvuuXz961/fJS5OPvnkXHnllfnSl76UffbZxzma0E0cddRRWbBgQXbu3JmrrroqK1asSPLSqROrVq3KoYce2tgBgd3yfx3Tw4YNa+yA7HVOhaJbWb58eT73uc/lPe95TyZNmpSRI0c2eiSgaPny5fnkJz+ZTZs25eijj07fvn3z+OOP5/7770/fvn1t6oRuxjH95mXFgm5l9OjR+eY3v5mf/vSnufTSS/Poo482eiSgaPTo0bntttsyYMCAbN68OR/5yEeybNmy9O3bN9u3b/cGBLoZx/Sbl7Cg2xk9enSuueaatLa2ZvDgwY0eB9gDjjzyyCxYsCAvvvhili1blsceeyxJ0qdPnwZPBvw+HNNvTk6Fott64YUX0r9//0aPAexBy5cvz1//9V/nsMMOy/Tp0/PHf/zHjR4JKHBMv7lYsaDbEhXQ81iRhJ7FMf3mYsUCgC7HiiT0LI7pNwdhAQAAlDkVCgAAKBMWAABAmbCgS9q2bVtmzJiRbdu2NXoUYA9wTEPP4pjmldhjQZe0ZcuWDB48OJs3b86gQYMaPQ5Q5JiGnsUxzSuxYgEAAJQJCwAAoKx3owfYXe3t7Xn66aczcODANDU1NXoc9pItW7bs8l+ge3NMQ8/imH7z6OjoSFtbWw455JA0N7/2mkS322Oxfv36DBs2rNFjAADAm8a6dety6KGHvuZjut2KxcCBAxs9ArCHzZw5s9EjAHvQwoULGz0CsIfs3LkzP/3pT1/Xe/BuFxZOf4Kep3///o0eAdiDevXq1egRgD3s9bwHt3kbAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAULbbYdHe3p4vf/nLecc73pF+/fpl+PDh+dKXvpQkmTJlSv7oj/4o++yzTw477LBMmzYt27dv73zujBkzMmrUqNx8880ZMWJEBg8enD//8z9PW1vbnvsbAQAAb7jeu/uEiy66KLNnz85Xv/rVjB07Nq2trXn00UeTJAMHDszcuXNzyCGH5Gc/+1k+//nPZ+DAgZk8eXLn81evXp2FCxfmO9/5Tp599tlMmDAhV1xxRWecvNy2bduybdu2zttbtmzZ3ZEBAIC9bLfCoq2tLV/72tdyzTXX5NOf/nSS5O1vf3vGjh2bJLn44os7HztixIhccMEFufXWW3cJi/b29sydOzcDBw5MkvzlX/5l7r777lcNi1mzZmXmzJm797cCAADeULt1KtSqVauybdu2nHDCCa94/2233ZZjjz02Q4cOzX777ZeLL744a9eu3eUxI0aM6IyKJHnrW9+ajRs3vuprXnTRRdm8eXPn17p163ZnZAAA4A2wW2ExYMCAV73vvvvuy8SJE3PyySfnO9/5TpYvX56pU6fmxRdf3OVxffr02eV2U1NT2tvbX/Xn9uvXL4MGDdrlCwAA6Fp2KywOP/zwDBgwIHfffffv3HfvvffmbW97W6ZOnZqjjz46hx9+eJ588sk9NigAANB17dYei/79+2fKlCmZPHly+vbtm2OPPTa//OUvs3Llyhx++OFZu3Ztbr311hxzzDH57ne/mzvvvHNvzQ0AAHQhu3252WnTpuX888/PJZdckne+850544wzsnHjxowbNy6TJk3KOeeck1GjRuXee+/NtGnT9sbMAABAF9PU0dHR0eghdseWLVsyePDgRo8B7EFXXnllo0cA9qD58+c3egRgD9m5c2eWL1+ezZs3/597nf3mbQAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoKx3owf4fW3evDmDBg1q9BjAHjBlypRGjwDsQddee22jRwD2kK1bt+b4449/XY+1YgEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAEBZw8JixowZGTVqVKNeHgAA2IOsWAAAAGW/d1h86EMfyhe/+MVMnjw5+++/f4YOHZoZM2Z03r927dqMHz8+++23XwYNGpQJEybkmWeeSZLMnTs3M2fOzMMPP5ympqY0NTVl7ty5r/g627Zty5YtW3b5AgAAupbSisW3vvWt7LvvvnnggQfy5S9/OX/3d3+Xu+66K+3t7Rk/fnw2bdqUe+65J3fddVfWrFmTM844I0lyxhln5Pzzz88RRxyR1tbWtLa2dt73crNmzcrgwYM7v4YNG1YZGQAA2At6V5581FFHZfr06UmSww8/PNdcc03uvvvuJMnPfvazPP74450hcNNNN+WII47IT37ykxxzzDHZb7/90rt37wwdOvQ1X+Oiiy7Keeed13l7y5Yt4gIAALqY0orFUUcdtcvtt771rdm4cWNWrVqVYcOG7RIAI0eOzFve8pasWrVqt16jX79+GTRo0C5fAABA11IKiz59+uxyu6mpKe3t7aWBAACA7mevXBXqne98Z9atW5d169Z1fu+RRx7Jr3/964wcOTJJ0rdv3+zcuXNvvDwAAPAG2yth8eEPfzjvete7MnHixCxbtiw//vGP86lPfSrHHXdcjj766CTJiBEj8vjjj2fFihX51a9+lW3btu2NUQAAgDfAXgmLpqamLFq0KEOGDMkHP/jBfPjDH85hhx2W2267rfMxp59+ej760Y+mpaUlBx54YObNm7c3RgEAAN4Av/dVoX74wx/+zvcWLlzY+efhw4dn0aJFr/r8fv365V/+5V9+35cHAAC6EL95GwAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGXCAgAAKBMWAABAmbAAAADKhAUAAFAmLAAAgDJhAQAAlAkLAACgTFgAAABlwgIAACgTFgAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZcICAAAoExYAAECZsAAAAMqEBQAAUCYsAACAMmEBAACUCQsAAKBMWAAAAGW9Gz3A7uro6EiSbNmypcGTAHvKtm3bGj0CsAdt3bq10SMAe8hzzz2X5P+/B38tTR2v51FdyPr16zNs2LBGjwEAAG8a69aty6GHHvqaj+l2YdHe3p6nn346AwcOTFNTU6PHAQCAHqujoyNtbW055JBD0tz82rsoul1YAAAAXY/N2wAAQJmwAAAAyoQFAABQJiwAAIAyYQEAAJQJCwAAoExYAAAAZf8PIEAW1Pgc9OcAAAAASUVORK5CYII=\n"},"metadata":{}}],"execution_count":47},{"cell_type":"markdown","source":"<b><h4><font color='green'>\n<hr style=\"border:10px solid red\"> </hr>\nAnswer 3: </b><br>\n\nIn successful,and even semi-successful, cases, the alignment heatmaps surface clear linguistic phenomena. On short, controlled sentences, the model reliably captures adjective–noun reordering across English and French: adjectives typically precede nouns in English but follow them in French. This appears as a local off-diagonal “cross” in the map (e.g., the red bus :: le bus rouge), where the target noun aligns to car and the target adjective aligns to red at earlier source positions. The maps also reveal split negation in French: a single English not distributes its attention over ne and pas, while the verb aligns tightly (can ↔ puisse).\n\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"-HY_LqQoJW_z"}},{"cell_type":"markdown","source":"\n\n\n<b><h4><font color='red'>\n<hr style=\"border:10px solid red\"> </hr>\nQuestion 4 (5 points): </b><br>\nWhat do you observe in the translations of the sentences below?\nWhat properties of language models does that illustrate?\nRead <a href='https://arxiv.org/abs/1802.05365'>[Peters et al., 2018]</a> and <a href='https://arxiv.org/abs/1810.04805'>[Devlin et al., 2018]</a>  to get some ideas.\n<ul>\n<b><h4><font color='red'>\n<li>$\\texttt{I did not mean to hurt you}$\n<li>$\\texttt{She is so mean}$\n</ul>\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>\n\n","metadata":{"id":"_jRkSDMjFXLH"}},{"cell_type":"code","source":"x1, _ = model.predict('we like')\nx2, _ = model.predict('like')\n\nprint(\"we like ->\",x1[:12])\nprint(\"like ->\",x2[:5])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-04T16:09:38.272190Z","iopub.execute_input":"2025-10-04T16:09:38.272774Z","iopub.status.idle":"2025-10-04T16:09:38.329220Z","shell.execute_reply.started":"2025-10-04T16:09:38.272750Z","shell.execute_reply":"2025-10-04T16:09:38.328653Z"}},"outputs":[{"name":"stdout","text":"we like -> nous aimons \nlike -> comme\n","output_type":"stream"}],"execution_count":157},{"cell_type":"markdown","source":"<b><h4><font color='green'>\n<hr style=\"border:10px solid red\"> </hr>\nAnswer 4: </b><br>\n\nIn my small test, I translated “we like” versus the single word “like” to highlight how modern language models rely on context. The different outputs show that the model uses the surrounding words to pick an appropriate meaning when context is present, and tends to fall back to a common, high-prior sense when context is missing. This matches the core claim of deep contextual word representations: a word’s vector should change with its sentence, which helps resolve ambiguity and capture sentence-level regularities\n\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>\n","metadata":{"id":"4AyxolVhJU5u"}},{"cell_type":"markdown","source":"### <b>6. Appendix:</b>\n<h4><b>6.1. GRU unit:</b></h4>\n<p style=\"text-align: justify;\">\nAs shown in Fig. 3, the GRU unit <a href='https://arxiv.org/abs/1406.1078'>[Cho et al., 2014]</a> is a simple RNN unit with two gates (reset and update):\n\n\\begin{equation}\n\\text{reset gate:}~~r_{t} = \\sigma \\big(U_{r}x_{t} + W_{r}h_{t-1} + b_r\\big)\n\\end{equation}\n\n\\begin{equation}\n\\text{update gate:}~~z_{t} = \\sigma \\big(U_{z}x_{t} + W_{z}h_{t-1} + b_z\\big)\n\\end{equation}\n\nThe candidate hidden state is computed as:\n\n\\begin{equation}\n\\hat{h}_{t} = \\mathrm{tanh} \\big(U_{h}x_{t} + W_{h} (r_t \\circ h_{t-1}) + b_h\\big)\n\\end{equation}\n\n<center>\n<img width='500px' src='https://1drv.ms/i/c/ae69638675180117/UQQXARh1hmNpIICuuYQBAAAAAEpvFJzwgjB2a3Y?width=703&height=489' />\n<br>\n<b>Figure 3:</b> GRU unit. Taken from <a href='http://colah.github.io/posts/2015-08-Understanding-LSTMs/'>Chris Olah's blog</a>.<br>\n</center>\n\nThe reset gate determines how much of the information from the previous time steps (stored in $h_{t-1}$) should be discarded.\nThe new hidden state is finally obtained by linearly interpolating between the previous hidden state and the candidate one:\n\n\\begin{equation}\nh_{t} = (1-z_t) \\circ {h}_{t-1} + z_t \\circ \\hat{h}_{t}\n\\end{equation}","metadata":{"id":"Z--prNCiJ0Un"}}]}