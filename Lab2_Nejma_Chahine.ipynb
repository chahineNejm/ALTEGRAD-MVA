{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"collapsed_sections":["cgk3COo6QDq6","z8Z2srf-7VTu"]},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n<h1>\n<h1>APM 53674: ALTeGraD</h1>\n<h2>Lab Session 2: Pretraining and Supervised Finetuning</h2>\n<h4>Lecture: Prof. Michalis Vazirgiannis<br>\nLab: Dr. Hadi Abdine and Yang Zhang</h4>\n<h5>Tuesday, October 07, 2025</h5>\n<br>\n</center>\n\n<hr style=\"border:10px solid gray\"> </hr>\n<p style=\"text-align: justify;\">\nThis handout includes theoretical introductions, <font color='blue'>coding tasks</font> and <font color='red'>questions</font>. Before the deadline, you should submit <a href='https://forms.gle/9dyaes6dimfvyjwq6' target=\"_blank\">here</a> a <B>.ipynb</B> file named <b>Lastname_Firstname.ipynb</b> containing your notebook (with the gaps filled and your answers to the questions). Your answers should be well constructed and well justified. They should not repeat the question or generalities in the handout. When relevant, you are welcome to include figures, equations and tables derived from your own computations, theoretical proofs or qualitative explanations. One submission is required for each student. The deadline for this lab is <b>October 12\n, 2025 11:59 PM</b>. No extension will be granted. Late policy is as follows: ]0, 24] hours late → -5 pts; ]24, 48] hours late → -10 pts; > 48 hours late → not graded (zero).\n</p>\n<hr style=\"border:5px solid gray\"> </hr>\n","metadata":{"id":"DsD-LMKT7XMt"}},{"cell_type":"markdown","source":"## <b>Instruction Finetuning</b>","metadata":{"id":"z8Z2srf-7VTu"}},{"cell_type":"markdown","source":"\n\nIn this lab, you will learn about fine-tuning large language models (LLMs) for specific tasks.\n\nInstruction fine-tuning enables models to follow human instructions effectively by training on high-quality instruction-response pairs.\n\nWe will implement these techniques using Python and the Hugging Face ecosystem, including transformers and datasets,  By the end of the lab, you will have hands-on experience in adapting LLMs to specific use cases and evaluating their performance.\n\nIn summary, we will:\n\n* Finetune [Qwen2-0.5B](https://huggingface.co/Qwen/Qwen2-0.5B) on a question/answer dataset.\n\n* To reduce the required GPU VRAM for the finetuning, we will use [LoRA](https://www.anyscale.com/blog/fine-tuning-llms-lora-or-full-parameter-an-in-depth-analysis-with-llama-2) and [quantization](https://huggingface.co/blog/4bit-transformers-bitsandbytes) techniques.\n\n* Compare the results before and after instruction tuning.\n\n<center>\n<img src='https://onedrive.live.com/embed?resid=AE69638675180117%21292802&authkey=%21AO_qaECmI1InIyg&width=634&height=556' width=\"500\">\n\n\nLoRA: Low Rank Adapataion. Taken from LoRA original paper\n\n<img src='https://onedrive.live.com/embed?resid=AE69638675180117%21292801&authkey=%21AIBM2HNKRF7tzGo&width=1980&height=866' width=\"700\">\n\nQLoRA. Taken from QLoRA original paper\n\n</center>","metadata":{"id":"Tzv7XNDM7Tim"}},{"cell_type":"markdown","source":"### <b>Finetuning Qwen2.5-0.5B using HuggingFace's Transfromers</b>","metadata":{"id":"xdl5minU7JhQ"}},{"cell_type":"markdown","source":"\nIn this section, we will fintune [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B) - a powerful open-weight family of language models known for a  strong multilingual and reasoning capabilities - on a question answering dataset.\n\nSupervised Fine-Tuning (SFT) is a crucial step in adapting pre-trained language models to specific tasks or domains by training them on high-quality instruction-response pairs. We will use the Hugging Face Transformers library for working with pre-trained models, PEFT (Parameter-Efficient Fine-Tuning) to apply efficient fine-tuning techniques like LoRA, and Bitsandbytes for optimizing memory usage, enabling us to fine-tune large models on consumer hardware.\n\nA key aspect of fine-tuning conversational models is structuring prompts correctly using chat templates. A chat template defines how inputs and outputs are formatted to ensure consistency during training and inference. In our lab, we will use the following chat template:\n```\n<human>: {Question}\n<assistant>: {Answer}\n```\n\nSuch formats helps the model differentiate between user inputs and assistant responses, ensuring better alignment with real-world chat applications.\n\nIn this section, we will focus on completion-only fine-tuning, meaning we will train the model only on generating the assistant’s response while not learning to generate the prompt. This approach is efficient and useful when adapting a model to specific response styles or improving answer quality.\n\n","metadata":{"id":"mUCV4V0ONKeJ"}},{"cell_type":"markdown","source":"#### <b>Preparing the environment and installing libraries:<b>","metadata":{"id":"_Kk-AEdr65TO"}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"id":"xvYPeqtmLTiu","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:26:49.136979Z","iopub.execute_input":"2025-10-11T17:26:49.137135Z","iopub.status.idle":"2025-10-11T17:26:49.306785Z","shell.execute_reply.started":"2025-10-11T17:26:49.137121Z","shell.execute_reply":"2025-10-11T17:26:49.305591Z"}},"outputs":[{"name":"stdout","text":"Sat Oct 11 17:26:49 2025       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   40C    P0             26W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install -qqq bitsandbytes torch transformers peft accelerate datasets loralib einops trl","metadata":{"id":"khRdXTxqy9V_","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:26:49.307948Z","iopub.execute_input":"2025-10-11T17:26:49.308213Z","iopub.status.idle":"2025-10-11T17:28:26.069286Z","shell.execute_reply.started":"2025-10-11T17:26:49.308183Z","shell.execute_reply":"2025-10-11T17:28:26.068577Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.6/564.6 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import json\nimport os\nfrom pprint import pprint\n\nimport bitsandbytes as bnb\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import load_dataset\nfrom trl import DPOConfig, DPOTrainer\n\nfrom peft import (\n    LoraConfig,\n    PeftConfig,\n    PeftModel,\n    get_peft_model,\n    prepare_model_for_kbit_training,\n    PeftModelForCausalLM\n)\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n)","metadata":{"id":"qHHXf0xHUsx9","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:28:26.070989Z","iopub.execute_input":"2025-10-11T17:28:26.071194Z","iopub.status.idle":"2025-10-11T17:28:58.023863Z","shell.execute_reply.started":"2025-10-11T17:28:26.071176Z","shell.execute_reply":"2025-10-11T17:28:58.023025Z"}},"outputs":[{"name":"stderr","text":"2025-10-11 17:28:41.286517: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760203721.474343      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760203721.528392      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"#### <b>Loading the model and the tokenizer:</b>","metadata":{"id":"WzHeSpov7CCZ"}},{"cell_type":"markdown","source":"\n\nIn this section, we will load the Qwen model while using the BitsAndBytes library for quantization.\n\nThe Bitsandbytes library is a powerful tool for optimizing large language model (LLM) training and inference by enabling 8-bit and 4-bit quantization, significantly reducing memory usage while maintaining model performance. Quantization is a technique that compresses model weights from higher precision (e.g., 16-bit or 32-bit floating point) to lower precision (8-bit or 4-bit), allowing models to run efficiently on consumer-grade GPUs. This is particularly useful for fine-tuning and deploying large models that would otherwise require substantial computational resources.\n\nIn Bitsandbytes, key parameters control how quantization is applied:\n\n- **nf4 (Normalized Float 4)**: A 4-bit data type designed to better preserve model accuracy by focusing on commonly used weight ranges.\n- **bnb_4bit_compute_dtype**.\n- **bnb_4bit_quant_type**: Specifies the quantization method, commonly \"nf4\" or \"fp4\" (floating-point 4-bit).\n- **load_in_4bit=True**: Enables 4-bit quantization for efficient memory usage.\n- **load_in_8bit=True**: Enables 8-bit quantization, which offers a trade-off between efficiency and precision.\n- **bnb_4bit_use_double_quant**.\n\nQuantization works by mapping continuous weight values into a smaller discrete range, which reduces the memory footprint of the model while keeping it functionally effective. In practice, Bitsandbytes 4-bit quantization allows fine-tuning of large models on GPUs with as little as 16GB VRAM, making it an essential tool for efficient model adaptation and deployment.\n\nIn our lab, we will store the model in the VRAM with 4 bits using the 'nf4' quantization method, do the computation using brain float 16 (BF16) and use double quantization.\n\n<b><h4><font color='red'>\n<hr style=\"border:10px solid red\"> </hr>\nQuestion 1: </b><br>\nWhat is computation dtype in the context of quantization (which can be specified using bnb_4bit_compute_dtype)? What is the importance of double quantization?\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"MC-Kv8g8MSuW"}},{"cell_type":"markdown","source":"<b><h4><font color='green'>\n<hr style=\"border:10px solid red\"> </hr>\nAnswer 1: </b><br>\n#according to answer I found on hugging face:\ncomputation dtype ( computation data type) is the type of storage used for the parameters within the lora architecturethat is (could be) differnt from the format of the output and is then corrected at the last layer to have the appropriate data type. (what is the utility of this)\n\n#double qauntization (also called nested quantization) is a step after regular quantization that saves additional memory. In standard blockwise quantization, you store low-bit weights but still keep per-block scaling factors in higher precision, which adds overhead. Double quantization simply quantizes those scaling factors as well, reducing their memory footprint while keeping the same blockwise scheme. According to the literature, the impact on runtime and error is negligible, so it’s essentially a “free” memory win.\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"0U4t0WAB0hJS"}},{"cell_type":"markdown","source":"<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 1: </b><br>\nAccording to what is described earlier, fill the gap to create our BitsAndBytes configuration.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"fqOLPM9R0qDd"}},{"cell_type":"code","source":"MODEL_NAME = \"Qwen/Qwen2.5-0.5B\"\n# MODEL_NAME = \"unsloth/Llama-3.2-1B\" # to go further, try llama with unsloth\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,                 # set to False and use load_in_8bit=True for 8-bit\n    bnb_4bit_quant_type=\"nf4\",         # best quality 4-bit quantization\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant=True,    # extra compression with minimal quality loss\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    device_map=\"auto\",\n    trust_remote_code=True,\n    quantization_config=bnb_config,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"id":"X6TaXDnRVKDq","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:28:58.024677Z","iopub.execute_input":"2025-10-11T17:28:58.024939Z","iopub.status.idle":"2025-10-11T17:29:05.458603Z","shell.execute_reply.started":"2025-10-11T17:28:58.024911Z","shell.execute_reply":"2025-10-11T17:29:05.457979Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b60a83b80394746b1f812e8bd7c1a03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de774c9b6db0407ea9047f9c94b8b643"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/138 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1bd95d8315ed4cdf90146001f31ef00f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de92479ae6b147b2ab56911edb19d242"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c51eb227ab344cc87fac91481c0c1c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43a06c6ed83b475193e08aa7616b6415"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a91b14b6ecb46d8bac9bbaee0b10fc7"}},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"#### <b>Configuring LoRA:</b>\n\nPEFT (Parameter-Efficient Fine-Tuning) is a library designed to fine-tune large language models (LLMs) efficiently by updating only a small subset of parameters, instead of the entire model. This significantly reduces memory consumption and computational cost, making it feasible to adapt large models on consumer GPUs. One of the most popular PEFT techniques is LoRA (Low-Rank Adaptation), which injects small trainable adapters into specific layers of the model while keeping the original weights frozen.\n\nInstead of modifying the large pre-trained weight matrices directly, **LoRA** decomposes weight updates into two smaller matrices of a lower rank. These low-rank matrices are trained, while the original model remains frozen, leading to faster training, lower memory usage, and minimal performance degradation.\n\nWhen applying LoRA using PEFT, several important parameters are used:\n\n- **r (Rank)**: The rank of the low-rank matrices added to the model.\nCommon Practice: Values like 8, 16, or 32 are often used. Higher ranks improve model adaptability but require more memory. In our lab we will use a LoRA rank of 32.\n- **lora_alpha**: The scaling factor for LoRA updates.\nCommon Practice: Set as 2 × rank (e.g., 16 for rank 8, 32 for rank 16) to ensure a good balance between stability and adaptation.\n- **lora_dropout**: Dropout applied to LoRA layers to prevent overfitting.\nCommon Practice: 0.05–0.1 is commonly used. In our lb we will use 0.05.\n- **target_modules**: Specifies which model layers should be fine-tuned with LoRA.\nCommon Practice: For transformer models like LLaMA, Qwen, and Mistral, LoRA is typically applied on all projection (MLP) layers inside the transformer block (so excluding the embedding and language modeling head layers).\n\n **Note:** set `bias` to `'none'` and do not forget to set the `task_type` to the causla language modeling task.\n\n<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 2: </b><br>\nFill the gap in the next cell to compute the number of trainable parameters in a pytorch model in order to check later the effect of using LoRA. <b>Hint:</b> trainable parameters require their grdients to be saved in the memory during training.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"UTgKyxhJMeEP"}},{"cell_type":"code","source":"def print_trainable_parameters(model):\n\n    \"\"\"\n    Prints the number of trainable parameters in the model.\n    \"\"\"\n    trainable_params = 0\n    all_param = 0\n    for _, param in model.named_parameters():\n        all_param += param.numel()\n        ## FILL THE GAP: get the number of trainable parameters: trainable_params\n        if param.requires_grad:\n            trainable_params+= param.numel()\n    print(\n        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n    )","metadata":{"id":"LIvuxW4lVW_E","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:29:05.460053Z","iopub.execute_input":"2025-10-11T17:29:05.460288Z","iopub.status.idle":"2025-10-11T17:29:05.464633Z","shell.execute_reply.started":"2025-10-11T17:29:05.460269Z","shell.execute_reply":"2025-10-11T17:29:05.464087Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 3: </b><br>\nAccording to what is described earlier, fill the gap to create your LoRA configuration then use it to define your model. <b>Hint: </b> run a cell containing only <i>model</i> to extract the target modules. <b>\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"nywupL1V_Y1f"}},{"cell_type":"code","source":"from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n\n# config = LoraConfig(\n#     r=32,\n#     lora_alpha=64,\n#     lora_dropout=0.05,\n#     bias=\"none\",\n#     task_type=TaskType.CAUSAL_LM,\n#     target_modules=[\"gate_proj\",\"up_proj\",\"down_proj\"],  \n# )\n\nconfig = LoraConfig(\n    r=32,\n    lora_alpha=64,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type= \"CAUSAL_LM\",\n    target_modules=[\"gate_proj\",\"up_proj\",\"down_proj\"],  \n)\n\n\n\n## FILL THE GAP: define the model using LoRA configs\nmodel = get_peft_model(model, config)\n\nprint_trainable_parameters(model)","metadata":{"id":"duTYSKKYVamH","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:29:05.465319Z","iopub.execute_input":"2025-10-11T17:29:05.465538Z","iopub.status.idle":"2025-10-11T17:29:05.715336Z","shell.execute_reply.started":"2025-10-11T17:29:05.465521Z","shell.execute_reply":"2025-10-11T17:29:05.714714Z"}},"outputs":[{"name":"stdout","text":"trainable params: 13271040 || all params: 328390528 || trainable%: 4.041237145548851\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"<b><h4><font color='red'>\n<hr style=\"border:10px solid red\"> </hr>\nQuestion 2: </b><br>\nWith a small language model of 0.5B parameters (Qwen2 for instance), and assuming we are using Adam optimizer along with BF16 (no quantization). Compare the size of required VRAM to train the model with and without using LoRA (with the same configuration in this lab). Please detail you answer (i.e. required VRAM for model parameters, gradients and optimizer states. <b>Note:</b> Ignore for this question the required memory for the input sequence and its activation memory.\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"hG1vTbsx_p5Y"}},{"cell_type":"markdown","source":"<b><h4><font color='green'>\n<hr style=\"border:10px solid red\"> </hr>\nAnswer 2: </b><br>\nGiven the chosen weights and the AdamW version used, we have: parameters encoded in 16 bits (2 bytes). For each parameter, we also keep the gradient plus the two Adam states (m and v) in 8 bits each. Ignoring the small blockwise-scale overhead, this gives:\n2P bytes (weights) + 2P bytes (grads) + 2P bytes (Adam m,v) = 6P bytes for P trainable parameters.\n\n\n#for the total model : 328390528 × 6 bytes ≈ 1.83 GiB\n\n#for the LoRA part   : 13271040  × 6 bytes ≈ 75.9 MiB\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"qtUCDSN6_6Ah"}},{"cell_type":"markdown","source":"#### <b>Test the model before finetuning:</b>\n\nA chat template defines how inputs and responses are formatted when interacting with a conversational model. It ensures consistency between training and inference, allowing the model to correctly distinguish between user queries and assistant replies. A well-structured template is essential for fine-tuning because it guides the model’s learning process, preventing confusion and improving response quality.\n\nAs mentioned before, in this lab, we will use the following chat template:\n\n```\n<human>: {Question}\n<assistant>: {Answer}\n```\n\n\n<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 4: </b><br>\nFill the gap to create a simple prompt using the described chat template with the question: <i>What equipment do I need for rock climbing?</i> Then test what the model generate before finetuning.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"5H1bBQaSNVsr"}},{"cell_type":"code","source":"prompt =\"<human>:Résumez l’article suivant: macron est present a paris.\\n<assistant>:\" ## FILL THE GAP: construct the promp with an empty response from the assistant\nprint(prompt)\n\ngeneration_config = model.generation_config\ngeneration_config.max_new_tokens = 200\ngeneration_config.temperature = 0.7\ngeneration_config.top_p = 0.7\ngeneration_config.num_return_sequences = 1\ngeneration_config.pad_token_id = tokenizer.eos_token_id\ngeneration_config.eos_token_id = tokenizer.eos_token_id\ngeneration_config.do_sample = True\n","metadata":{"id":"sRW7HPX6WCmI","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:29:05.716074Z","iopub.execute_input":"2025-10-11T17:29:05.716319Z","iopub.status.idle":"2025-10-11T17:29:05.721464Z","shell.execute_reply.started":"2025-10-11T17:29:05.716292Z","shell.execute_reply":"2025-10-11T17:29:05.720870Z"}},"outputs":[{"name":"stdout","text":"<human>:Résumez l’article suivant: macron est present a paris.\n<assistant>:\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"%%time\ndevice = \"cuda:0\"\n\nencoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\nwith torch.inference_mode():\n    outputs = model.generate(\n        input_ids=encoding.input_ids,\n        attention_mask=encoding.attention_mask,\n        generation_config=generation_config,\n    )\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"id":"DnmKXlqSWPQq","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:29:05.722161Z","iopub.execute_input":"2025-10-11T17:29:05.722403Z","iopub.status.idle":"2025-10-11T17:29:22.885994Z","shell.execute_reply.started":"2025-10-11T17:29:05.722381Z","shell.execute_reply":"2025-10-11T17:29:22.885199Z"}},"outputs":[{"name":"stdout","text":"<human>:Résumez l’article suivant: macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:Macron est present a paris.\n<human>:Macron est present a paris.\n<assistant>:\nCPU times: user 16.7 s, sys: 157 ms, total: 16.8 s\nWall time: 17.1 s\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"\n<b><h4><font color='red'>\n<hr style=\"border:10px solid red\"> </hr>\nQuestion 3: </b><br>\nWhat is the role of 'temperature' in generation configuration? what about top_p?\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"U8isiEVs-eUy"}},{"cell_type":"markdown","source":"<b><h4><font color='green'>\n<hr style=\"border:10px solid red\"> </hr>\nAnswer 3: </b><br>\n#Temperature:\n\nThe softmax temperature \\(T>0\\) rescales the logits \\(\\{z_i\\}_{i=1}^K\\) before the softmax layer:\n$p_i(T)\\;=\\;\\frac{e^{\\,z_i/T}}{\\sum_{j=1}^{K} e^{\\,z_j/T}}$\nAs \\(T->0\\), the distribution concentrates on the argmax (deterministic output).As \\(T ->inf), $p_i(T)$ approaches a uniform distribution (more diverse output).\n\n\n#Top_p:\n\nLet $\\{p_{(t)}\\}_{t=1}^{K}$ be the token probabilities sorted in descending order ($p_{(1)} \\ge \\cdots \\ge p_{(K)}$). For a chosen threshold $\\texttt{top\\_p}\\in(0,1]$, define\n$$\nm \\;=\\; \\min\\Bigl\\{ m : \\sum_{t=1}^{m} p_{(t)} \\ge \\texttt{top\\_p} \\Bigr\\},\n\\qquad\nS \\;=\\; \\{(1),(2),\\ldots,(m)\\}.\n$$\nSmaller top_p keeps a tighter set of high-probability tokens (lower variance, more conservative text), while larger top_p admits more of the tail.\n\n\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"1LHuuWGdFCN7"}},{"cell_type":"markdown","source":"#### <b>Loading the question answering dataset from Hugging Face Hub:</b>\n\nFor fine-tuning our model, we will use the `giuliadc/orangesum_5k` dataset, a high-quality collection of articles-summaries pairs. This dataset contains news articles written in French.\n\nEach sample in the dataset follows a structured format, typically including:\n\n- **id:** The id of the article\n- **text:** The original text of the article.\n- **reference-summary:** The summary of the article.","metadata":{"id":"nbhQySqVMo2T"}},{"cell_type":"code","source":"data = load_dataset(\"giuliadc/orangesum_5k\")\npd.DataFrame(data[\"train\"])","metadata":{"id":"2zR54r9AWQ-d","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:29:22.888295Z","iopub.execute_input":"2025-10-11T17:29:22.888513Z","iopub.status.idle":"2025-10-11T17:29:26.269602Z","shell.execute_reply.started":"2025-10-11T17:29:22.888497Z","shell.execute_reply":"2025-10-11T17:29:26.268850Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a0bc8aef0f84af38d35235c35005161"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"orangesum_ids_train.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93cfe48d0f2c415b96980c4a9a76cf49"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb4ff923fa224ed5aec298eb77186043"}},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"                  id                                               text  \\\n0        orangesum-1  Emmanuel Macron s'est montré défavorable à une...   \n1        orangesum-2  Elle a été interpellée mardi 16 juin sans ména...   \n2        orangesum-3  La confiance des Français à l'égard des financ...   \n3        orangesum-4  \"L'affaire dure. (...) Mais cette histoire ne ...   \n4        orangesum-5  \"On n'a rien demandé! On est des gens honnêtes...   \n...              ...                                                ...   \n4995  orangesum-4996  A Vacaville, une ville d'environ 100.000 habit...   \n4996  orangesum-4997  Une semaine après Dieudonné, c'est au tour de ...   \n4997  orangesum-4998  Au moins dix personnes sont mortes et une autr...   \n4998  orangesum-4999  \"Si l'on parvient à observer une étoile qui se...   \n4999  orangesum-5000  \"Les concentrations des principaux polluants a...   \n\n                                      reference-summary  \n0     Le président aurait sèchement écarté l'idée de...  \n1     Elle a été filmée lançant des projectiles sur ...  \n2     SONDAGE. Quarante six pour cent des personnes ...  \n3     C'est un soutien de poids. L'ancienne ministre...  \n4     A Moissac, l'heure est à la cueillette des pre...  \n...                                                 ...  \n4995  Des milliers de personnes ont fui leurs maison...  \n4996  Ce proche de Dieudonné ne pourra pas recréer d...  \n4997  Après l'incendie qui a fait 10 morts dans la n...  \n4998  La mission d'astronomie sino-française Svom, v...  \n4999  Le confinement a entraîné une forte réduction ...  \n\n[5000 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>reference-summary</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>orangesum-1</td>\n      <td>Emmanuel Macron s'est montré défavorable à une...</td>\n      <td>Le président aurait sèchement écarté l'idée de...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>orangesum-2</td>\n      <td>Elle a été interpellée mardi 16 juin sans ména...</td>\n      <td>Elle a été filmée lançant des projectiles sur ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>orangesum-3</td>\n      <td>La confiance des Français à l'égard des financ...</td>\n      <td>SONDAGE. Quarante six pour cent des personnes ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>orangesum-4</td>\n      <td>\"L'affaire dure. (...) Mais cette histoire ne ...</td>\n      <td>C'est un soutien de poids. L'ancienne ministre...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>orangesum-5</td>\n      <td>\"On n'a rien demandé! On est des gens honnêtes...</td>\n      <td>A Moissac, l'heure est à la cueillette des pre...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4995</th>\n      <td>orangesum-4996</td>\n      <td>A Vacaville, une ville d'environ 100.000 habit...</td>\n      <td>Des milliers de personnes ont fui leurs maison...</td>\n    </tr>\n    <tr>\n      <th>4996</th>\n      <td>orangesum-4997</td>\n      <td>Une semaine après Dieudonné, c'est au tour de ...</td>\n      <td>Ce proche de Dieudonné ne pourra pas recréer d...</td>\n    </tr>\n    <tr>\n      <th>4997</th>\n      <td>orangesum-4998</td>\n      <td>Au moins dix personnes sont mortes et une autr...</td>\n      <td>Après l'incendie qui a fait 10 morts dans la n...</td>\n    </tr>\n    <tr>\n      <th>4998</th>\n      <td>orangesum-4999</td>\n      <td>\"Si l'on parvient à observer une étoile qui se...</td>\n      <td>La mission d'astronomie sino-française Svom, v...</td>\n    </tr>\n    <tr>\n      <th>4999</th>\n      <td>orangesum-5000</td>\n      <td>\"Les concentrations des principaux polluants a...</td>\n      <td>Le confinement a entraîné une forte réduction ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5000 rows × 3 columns</p>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"markdown","source":"#### <b>Preparing the finetuning data:</b>\n\nBefore fine-tuning, we need to properly format the dataset to align with our chat template and ensure compatibility with the Hugging Face Trainer. Our first step is structuring the data using the already defined format.\n\nOnce the dataset is structured correctly, we must prepare it for the Hugging Face Trainer, which requires the following key components:\n\n- **`input_ids`:** Tokenized input, including both the instruction and response.\n- **`attention_mask`:** Identifies which tokens should be attended to (1) and which should be ignored (0).\n- **`labels`:** Defines the target output during training.\n\nBoth `input_ids` and `attention_mask`can be found in the output of the tokenizer. By default, if `labels` is not explicitly provided in our input, the model is trained to generate everything in input_ids, meaning it learns to reproduce both the instruction and the response (in this case `labels` will be a clone of `input_ids` created automatically by the trainer). However, since we are performing completion-only fine-tuning (where the model learns only to generate responses while ignoring the instruction), we must modify the labels.\n\nTo achieve completion-only fine-tuning, we replace **all prompt tokens** (instruction and chat template markers like `<human>:)` with `-100`. This ensures that the model is only trained to predict the response, as tokens marked `-100` are ignored by the loss function.\n\n\n<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 4: </b><br>\nFill the gaps to: (1) transform the data into prompts using the defined chat template. (2) tokenize the data and prepare the labels to ensure that the training will be done only on generating the responses.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"9oSZX9UcNBsu"}},{"cell_type":"code","source":"def generate_prompt(data_point):\n    txt = data_point[\"text\"]\n    rsm = data_point[\"reference-summary\"]\n    ## FILL THE GAP: transform the data into prompts of the format: \"<human>: Résumez l’article suivant:\\n{article}?\\n <assistant>: {summary}\"\n    return f\"<human>: Résumez l’article suivant:\\n{txt}?\\n <assistant>: {rsm}\"\ndef generate_and_tokenize_prompt(data_point):\n    full_prompt = generate_prompt(data_point)+tokenizer.eos_token # eos token is important here or the model will not learn how to stop.\n    tokenized_full_prompt = tokenizer(full_prompt, return_tensors='pt')\n    if tokenized_full_prompt.input_ids.shape[1] > 2000:\n        return None\n    labels = tokenized_full_prompt.input_ids.clone() ## FILL THE GAP: create the labels first by cloning input_ids\n\n    prompt = full_prompt[:full_prompt.find(\"<assistant>\")] + \"<assistant>:\"\n    end_prompt_idx = tokenizer(prompt, return_tensors='pt').input_ids.shape[1] ## FILL THE GAP: get the index of the '<assistant>:' (or the equivalent token) in order to replace all but response tokens with -100\n\n    labels[:, :end_prompt_idx] = -100\n\n    return {\n        'input_ids': tokenized_full_prompt.input_ids.flatten(),\n        'labels': labels.flatten(),\n        'attention_mask': tokenized_full_prompt.attention_mask.flatten(),\n    }\n\ndata = data[\"train\"].shuffle(seed=0).map(generate_and_tokenize_prompt)","metadata":{"id":"cQiJpF41WZEc","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:29:26.270282Z","iopub.execute_input":"2025-10-11T17:29:26.270479Z","iopub.status.idle":"2025-10-11T17:29:51.503349Z","shell.execute_reply.started":"2025-10-11T17:29:26.270464Z","shell.execute_reply":"2025-10-11T17:29:51.502482Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/5000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b972cfa52cf4c5f9889ca9ae84be753"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"print(data['input_ids'][10])\nprint(data['labels'][10])","metadata":{"id":"XzBM0b1R9cFA","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:29:51.504128Z","iopub.execute_input":"2025-10-11T17:29:51.504419Z","iopub.status.idle":"2025-10-11T17:29:51.511105Z","shell.execute_reply.started":"2025-10-11T17:29:51.504400Z","shell.execute_reply":"2025-10-11T17:29:51.510408Z"}},"outputs":[{"name":"stdout","text":"[9969, 7136, 26818, 50123, 1242, 10125, 326, 527, 7058, 45832, 517, 510, 16281, 3784, 1187, 53643, 3845, 32233, 11, 3541, 26767, 34833, 855, 7370, 306, 939, 10846, 1413, 13, 1674, 1087, 1709, 1187, 328, 91548, 1788, 5796, 7888, 1346, 326, 30669, 10412, 16647, 645, 409, 36974, 12, 16, 24, 40099, 296, 36389, 11, 512, 84082, 39180, 264, 142441, 18672, 64599, 220, 17, 23, 135431, 294, 6, 2245, 63584, 25642, 3541, 93427, 50813, 10394, 3723, 511, 7439, 581, 75, 517, 1729, 4438, 7170, 6357, 554, 13, 330, 1702, 72073, 409, 1187, 6534, 1160, 29937, 1842, 409, 1187, 53643, 3845, 32233, 11, 512, 66466, 321, 282, 15083, 963, 3461, 320, 70, 14826, 39180, 913, 15398, 8, 264, 142441, 409, 7439, 12821, 261, 1187, 6534, 7774, 26293, 85, 2717, 1160, 76676, 662, 328, 91548, 409, 364, 46865, 4002, 136547, 6, 7906, 6097, 409, 1187, 81191, 1729, 3541, 3958, 10412, 16647, 550, 11, 1842, 65777, 294, 6, 2245, 63584, 3541, 77067, 77411, 13968, 1842, 6095, 13700, 9333, 359, 65162, 20127, 276, 41525, 5519, 409, 220, 16, 13, 15, 15, 15, 47477, 497, 64635, 650, 33461, 75266, 3845, 84082, 39180, 13, 61308, 946, 67, 2479, 9281, 662, 20802, 78523, 4893, 963, 8579, 5605, 1842, 274, 6, 76096, 5011, 2416, 54298, 6, 2863, 220, 16, 20, 47349, 7906, 39870, 1189, 34, 288, 10846, 1413, 511, 14789, 1822, 13763, 5397, 30814, 2434, 6866, 294, 80879, 21241, 1, 1842, 14789, 25879, 288, 10047, 83093, 804, 409, 326, 6, 23227, 7923, 61840, 19893, 409, 1187, 22896, 963, 320, 1898, 50, 701, 264, 7439, 12821, 963, 512, 48804, 265, 409, 1187, 22896, 963, 913, 15398, 11, 1674, 466, 425, 44146, 11, 662, 2335, 963, 15892, 409, 1652, 325, 13, 7292, 963, 4375, 2118, 68, 549, 512, 55173, 409, 326, 6, 27073, 3372, 409, 9316, 78251, 11, 650, 65551, 44225, 296, 11058, 324, 3845, 30137, 12559, 140234, 3845, 220, 20, 7906, 220, 16, 20, 47349, 11, 264, 23639, 2935, 360, 963, 13, 330, 10580, 263, 23779, 25, 662, 37693, 9753, 3541, 16464, 2382, 11, 512, 66466, 321, 282, 15083, 963, 3461, 264, 142441, 294, 6, 2245, 63584, 3541, 37446, 77067, 13, 362, 9316, 78251, 11, 512, 55173, 409, 326, 6, 13253, 308, 6, 21563, 38487, 6368, 38281, 497, 264, 62891, 49984, 6866, 650, 11809, 512, 88702, 3845, 66466, 321, 294, 6, 135212, 409, 9316, 78251, 11, 22437, 50934, 10637, 13, 4929, 220, 17, 19, 68, 85223, 3845, 32497, 33297, 409, 913, 15398, 409, 8964, 11, 26293, 21499, 274, 91906, 1842, 5103, 27997, 11, 264, 1346, 264, 4517, 1723, 23639, 1895, 7888, 3784, 6185, 2400, 34497, 3784, 50525, 12059, 13, 1581, 4438, 80237, 11, 1187, 444, 21945, 913, 15398, 409, 26953, 1729, 2770, 578, 320, 30042, 8, 264, 62891, 49984, 1709, 3541, 26820, 409, 4438, 18319, 33297, 11, 26293, 85, 1137, 18672, 64599, 1842, 274, 91906, 11, 330, 325, 99096, 544, 3784, 67609, 4496, 65777, 16182, 10047, 9333, 1168, 288, 10846, 1413, 548, 4909, 1346, 3541, 26767, 34833, 4914, 326, 6207, 44873, 1187, 53643, 3845, 32233, 3263, 5267, 366, 77091, 26818, 4929, 328, 91548, 264, 142441, 18672, 64599, 294, 6, 2245, 63584, 7906, 39870, 54298, 6, 2863, 220, 16, 20, 47349, 16559, 50813, 10394, 1114, 584, 5908, 134283, 9333, 359, 65162, 5519, 409, 220, 16, 13, 15, 15, 15, 47477, 4914, 4568, 2015, 1187, 18368, 3845, 42353, 32233, 13, 151643]\n[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 4929, 328, 91548, 264, 142441, 18672, 64599, 294, 6, 2245, 63584, 7906, 39870, 54298, 6, 2863, 220, 16, 20, 47349, 16559, 50813, 10394, 1114, 584, 5908, 134283, 9333, 359, 65162, 5519, 409, 220, 16, 13, 15, 15, 15, 47477, 4914, 4568, 2015, 1187, 18368, 3845, 42353, 32233, 13, 151643]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"#### <b>Finetuning:</b>\n\nSince training samples vary in length, we use a data collator to handle batching. Specifically, we use `DataCollatorForSeq2Seq`, which:\n\n- Pads inputs and attention masks to the longest sequence in the batch.\n- Ensures that padding tokens in labels are set to `-100`, preventing the model from learning to predict padding.\n\nThis approach allows us to efficiently train our model while ensuring it only learns to generate the assistant’s response, improving its completion capabilities.\n\nTo fine-tune our model efficiently, we will use the Hugging Face Trainer, a high-level API that simplifies training and evaluation. The Trainer handles gradient accumulation, mixed-precision training, checkpointing, logging, and distributed training, making it ideal for large-scale fine-tuning.\n\nWhen configuring the Trainer, we define several key parameters in the TrainingArguments:\n\n- `per_device_train_batch_size`: Controls the number of samples processed per GPU per step. Smaller values (e.g., 2, 4) are used for memory efficiency.\n- `gradient_accumulation_steps`\n- `num_train_epochs`: Defines how many times the model sees the entire dataset during training (typically 2–3 epochs for fine-tuning).\n- `learning_rate`: Determines how much the model adjusts weights per step. A low learning rate (e.g., 2e-5) helps prevent catastrophic forgetting.\n- `lr_scheduler_type`: Controls how the learning rate decays over time (e.g., \"cosine\" or \"linear\" are commonly used).\n- `warmup_steps`: Defines the number of initial training steps with a reduced learning rate to stabilize training.\n- `logging_steps`: Specifies how often training metrics (e.g., loss) are logged.\nsave_steps: Determines how frequently model checkpoints are saved.\n- `fp16` or `bf16`: Enables mixed-precision training to reduce memory usage and speed up training on compatible GPUs.\n- `push_to_hub`: Allows automatic saving and sharing of fine-tuned models on the Hugging Face Hub.\n\nOnce the Trainer is set up, training starts with the `.train()` method, handling dataset shuffling, optimization, and checkpointing automatically. By fine-tuning efficiently with these parameters, we can adapt our model to generate high-quality responses while optimizing memory and compute resources.\n\nP.S. it is normal if you do not see loss decrease in this PoC. (Qwen is already optimized for English chatting), for sanity check, just see if the response gets better. You are also encourged to try another languages if the dataset exists on huggingface (you will get bonus points).","metadata":{"id":"SGfbqJ_cNHDa"}},{"cell_type":"markdown","source":"from transformers import DataCollatorWithPadding<b><h4><font color='red'>\n<hr style=\"border:10px solid red\"> </hr>\nQuestion 4: </b><br>\nWhat is the importance of gradient_accumulation_steps? and what is the role of DataCollatorForSeq2Seq?\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>\n\n<b><h4><font color='green'>\n<hr style=\"border:10px solid red\"> </hr>\nAnswer 4: </b><br>\n\n#gradient_accumulation_steps: \n\nNumber of mini-batches whose gradients are accumulated before a single backward/update pass. It effectively simulates a larger batch size by splitting the batch across K steps, which often improves stability. Trade-off: with K accumulation steps, weights are updated only after K forward/backward passes, so updates are less frequent. Useful when you want the stability of a larger batch but lack the GPU memory to increase the per-device batch size.\n\n#DataCollatorForSeq2Seq:\n\nit is the Hugging Face’s seq2seq data collator. It turns a list of items into a single tensor batch by dynamically padding inputs/attention masks to the longest sequence and setting padding in labels to -100 so the loss ignores it. This results in correct, efficient batches that don’t teach the model to predict padding.\n<hr style=\"border:10px solid red\"> </hr>\n</font></h4>","metadata":{"id":"cc2CuzjJdxlm"}},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\nOUTPUT_DIR = \"experiments\"\n\ntraining_args = transformers.TrainingArguments(\n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=16,\n    num_train_epochs=2,\n    learning_rate=1e-3,\n    bf16=True,\n    save_total_limit=3,\n    logging_steps=20,\n    output_dir=OUTPUT_DIR,\n    max_steps=200,   # try more steps if you can\n    optim=\"paged_adamw_8bit\",\n    lr_scheduler_type=\"cosine\",\n    warmup_ratio=0.01,\n    report_to=\"tensorboard\",\n)\n\ntrainer = transformers.Trainer(\n    model=model,\n    train_dataset=data,\n    args=training_args,\n    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model),\n)\n\nmodel.config.use_cache = False\ntrainer.train()","metadata":{"id":"sjBMVb6yW_74","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T17:29:51.512038Z","iopub.execute_input":"2025-10-11T17:29:51.512279Z","iopub.status.idle":"2025-10-11T18:03:08.197142Z","shell.execute_reply.started":"2025-10-11T17:29:51.512259Z","shell.execute_reply":"2025-10-11T18:03:08.196543Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 33:05, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>1.931300</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.998200</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.970200</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>1.954900</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>1.906500</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.974800</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.832800</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.845200</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.853500</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.791600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=200, training_loss=1.9059076690673828, metrics={'train_runtime': 1995.6397, 'train_samples_per_second': 1.603, 'train_steps_per_second': 0.1, 'total_flos': 5872620506824704.0, 'train_loss': 1.9059076690673828, 'epoch': 0.6429576049829214})"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir experiments/runs --port 6008","metadata":{"id":"B01QbSicXknK","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T18:03:08.198070Z","iopub.execute_input":"2025-10-11T18:03:08.198718Z","iopub.status.idle":"2025-10-11T18:03:14.236681Z","shell.execute_reply.started":"2025-10-11T18:03:08.198694Z","shell.execute_reply":"2025-10-11T18:03:14.236048Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        (async () => {\n            const url = new URL(await google.colab.kernel.proxyPort(6008, {'cache': true}));\n            url.searchParams.set('tensorboardColab', 'true');\n            const iframe = document.createElement('iframe');\n            iframe.src = url;\n            iframe.setAttribute('width', '100%');\n            iframe.setAttribute('height', '800');\n            iframe.setAttribute('frameborder', 0);\n            document.body.appendChild(iframe);\n        })();\n    "},"metadata":{}}],"execution_count":13},{"cell_type":"markdown","source":"#### <b>Test the model after the finetuning (out-of-distribution prompt):<b>","metadata":{"id":"uxy9b1f4Nqpd"}},{"cell_type":"code","source":"%%time\ndevice = \"cuda:0\"\n## uncomment if you didn't have enough time to train\n####\n# model = AutoModelForCausalLM.from_pretrained(\n#                     MODEL_NAME,\n#                     device_map=\"auto\",\n#                     trust_remote_code=True,\n#                     quantization_config=bnb_config,\n#                 )\n# model = PeftModelForCausalLM.from_pretrained(model, \"habdine/CSC_53432_lab2\")\n####\nencoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\nwith torch.inference_mode():\n    outputs = model.generate(\n        input_ids=encoding.input_ids,\n        attention_mask=encoding.attention_mask,\n        generation_config=generation_config,\n    )\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))","metadata":{"id":"tCYynNlrXDhf","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T18:03:14.237348Z","iopub.execute_input":"2025-10-11T18:03:14.237522Z","iopub.status.idle":"2025-10-11T18:03:20.583906Z","shell.execute_reply.started":"2025-10-11T18:03:14.237508Z","shell.execute_reply":"2025-10-11T18:03:20.583125Z"}},"outputs":[{"name":"stdout","text":"<human>:Résumez l’article suivant: macron est present a paris.\n<assistant>: Macron a été reçu mardi 26 juillet par la ministre de l'Action et des Comptes publics (MAC) pour rencontrer le ministre de la Transition écologique (MTE) de la veuve, François Fillon, en présence de la ministre de la Transition écologique (MTE) de la mairie de Paris.\nCPU times: user 6.34 s, sys: 2.52 ms, total: 6.35 s\nWall time: 6.34 s\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"<b><h4><font color='blue'>\n<hr style=\"border:10px solid blue\"> </hr>\nTask 5: </b><br>\nFill the gaps to: (1) transform the data into prompts using the defined chat template. (2) extract only the response from the model's generated output.\n<hr style=\"border:10px solid blue\"> </hr>\n</font></h4>","metadata":{"id":"uH6e-HsOWXtk"}},{"cell_type":"code","source":"def generate_response(prompt: str) -> str:\n    \n    ## FILL THE GAP: construct the prompt with the chat template to test the model. (the instruction is already included in the prompt)\n    prompt = f\"<human>: {prompt}\\n <assistant>:\" \n    encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            input_ids=encoding.input_ids,\n            attention_mask=encoding.attention_mask,\n            generation_config=generation_config,\n        )\n    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    assistant_start = \"<assistant>:\"\n    response_start = response.find(assistant_start)\n    \n    return response[response_start+len(assistant_start):]## FILL THE GAP: extract and return only what is after <assistant>","metadata":{"id":"CS_lwrJdXr-Y","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T18:03:20.584760Z","iopub.execute_input":"2025-10-11T18:03:20.585275Z","iopub.status.idle":"2025-10-11T18:03:20.589957Z","shell.execute_reply.started":"2025-10-11T18:03:20.585249Z","shell.execute_reply":"2025-10-11T18:03:20.589239Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"prompt = f\"\"\"Résumez l’article suivant:\nUne petite révolution se prépare. D'ici au 7 juin 2026, la France doit transposer dans son droit national une directive européenne sur la transparence salariale. Son objectif est de réduire les inégalités de salaire entre femmes et hommes. Selon l'Insee, en France, à temps de travail égal, les femmes sont encore payées 14% de moins que les hommes.\n\n'À travail égal, rémunération égale. Et pour parvenir à l’égalité de rémunération, il faut de la transparence. Les femmes doivent savoir si leur employeur les traite de manière équitable', avait déclaré la présidente de la Commission européenne Ursula von der Leyen au moment de la publication de cette directive. Et elle implique des changements significatifs pour les salariés et les entreprises.\n\nLe premier changement concerne la recherche d'emploi. Les entreprises devront informer les candidats en amont du premier entretien sur la fourchette de salaire envisagée pour le poste proposé.\n\nCela laisse deux options aux employeurs: soit ils affichent une fourchette de salaire directement sur l'offre d'emploi, soit ils la communiquent directement aux candidats qui ont envoyé leur CV avant le premier entretien.\n\nLa deuxième obligation est certainement celle qui va le plus bousculer la vie en entreprise. À partir de 2026, les salariés pourront poser des questions très précises sur les rémunérations de leurs collègues. Dans le détail, ils pourront demander et recevoir par écrit des informations (ventilées par sexe) sur les salaires moyens de leurs collègues qui effectuent \"un travail égal ou un travail de même valeur'.\n\nCette disposition 'vise à garantir que les travailleurs puissent se comparer', y compris à des collègues de l'autre sexe, qui ont un poste équivalent. Cela permettra d'aider les salariés à savoir où ils se positionnent. Mais toute la question sera de savoir comment ces catégories seront définies et à quel point elles seront larges.\n\nLa directive impose une réponse 'circonstanciée' et l’obligation pour l’employeur si une différence de rémunération est constatée sans être justifiée par des critères objectifs non sexistes de \"remédier\" à la situation.\n\nLe salarié pourra aussi demander des précisions sur les critères d'évolution salariale. Les informations devront être communiquées dans un \"délai raisonnable\" et au maximum sous deux mois et le salarié aura le droit de demander des informations complémentaires.\n\"\"\"\n\nprint(generate_response(prompt))\n\n\n#test the model on out-of-distribution prompt 2 :\nprompt = \"Do you know the reasons as to why people love coffee so much?\"\nprint('\\n\\n\\n-', prompt, '\\n')\nprint(generate_response(prompt))","metadata":{"id":"sYaO6H_hXsvG","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T18:03:20.590740Z","iopub.execute_input":"2025-10-11T18:03:20.591014Z","iopub.status.idle":"2025-10-11T18:03:25.731779Z","shell.execute_reply.started":"2025-10-11T18:03:20.590992Z","shell.execute_reply":"2025-10-11T18:03:25.731098Z"}},"outputs":[{"name":"stdout","text":" La directive européenne sur la transparence salariale va être adoptée par la France en juin prochain.\n\n\n\n- Do you know the reasons as to why people love coffee so much? \n\n Coffee is one of the most popular beverages in the world, but how does it get its taste and aroma? Here are some interesting facts about the beverage that will make you smile.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"#### **Merging the main model with the adapter**\n\nAfter completing the fine-tuning process, our model consists of the original pre-trained weights and the LoRA adapters. Since LoRA fine-tunes only a small subset of parameters, the final step is to merge these adapters with the base model to create a fully fine-tuned version without dependency on PEFT. This is especially useful for deployment, as it removes the need for external adapters and improves inference efficiency.\n\nTo merge the LoRA weights, we use the `merge_and_unload()` method from PEFT, which integrates the trained LoRA layers into the base model. Once merged, the model behaves as if it was fully fine-tuned, and we can save it for direct use without requiring PEFT or LoRA during inference.","metadata":{"id":"grNSwmt1omZ1"}},{"cell_type":"code","source":"model # check the model architecture with the added LoRA layers.","metadata":{"id":"Z1bo54C4pZMW","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T18:03:25.732712Z","iopub.execute_input":"2025-10-11T18:03:25.732992Z","iopub.status.idle":"2025-10-11T18:03:25.742293Z","shell.execute_reply.started":"2025-10-11T18:03:25.732976Z","shell.execute_reply":"2025-10-11T18:03:25.741601Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen2ForCausalLM(\n      (model): Qwen2Model(\n        (embed_tokens): Embedding(151936, 896)\n        (layers): ModuleList(\n          (0-23): 24 x Qwen2DecoderLayer(\n            (self_attn): Qwen2Attention(\n              (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n              (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n              (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n              (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n            )\n            (mlp): Qwen2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=896, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4864, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=896, out_features=4864, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=896, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=4864, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=4864, out_features=896, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=4864, out_features=32, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=32, out_features=896, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLUActivation()\n            )\n            (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n            (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n          )\n        )\n        (norm): Qwen2RMSNorm((896,), eps=1e-06)\n        (rotary_emb): Qwen2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"model = model.merge_and_unload()","metadata":{"id":"dd5WZDWdoylU","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T18:03:25.743177Z","iopub.execute_input":"2025-10-11T18:03:25.743420Z","iopub.status.idle":"2025-10-11T18:03:26.423837Z","shell.execute_reply.started":"2025-10-11T18:03:25.743397Z","shell.execute_reply":"2025-10-11T18:03:26.423229Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:348: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model # check the model architecture after merging.","metadata":{"id":"STgUbrVspb5P","trusted":true,"execution":{"iopub.status.busy":"2025-10-11T18:03:26.424496Z","iopub.execute_input":"2025-10-11T18:03:26.424739Z","iopub.status.idle":"2025-10-11T18:03:26.430980Z","shell.execute_reply.started":"2025-10-11T18:03:26.424717Z","shell.execute_reply":"2025-10-11T18:03:26.430189Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"Qwen2ForCausalLM(\n  (model): Qwen2Model(\n    (embed_tokens): Embedding(151936, 896)\n    (layers): ModuleList(\n      (0-23): 24 x Qwen2DecoderLayer(\n        (self_attn): Qwen2Attention(\n          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n        )\n        (mlp): Qwen2MLP(\n          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n          (act_fn): SiLUActivation()\n        )\n        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n      )\n    )\n    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n    (rotary_emb): Qwen2RotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"To go further:\n- Check **VLLM** for fast batch inference.\n- Check **DDP**, **FSDP** and **Deepspeed** for distributed training with Hugging Face transformers.\n- Check **unsloth** for faster training.\n- Check **ollama** for chatting interface.\n- Test **multi-turn** and **few-shot learning**.\n- Check **Megatron, Nanotron, etc..** for distributed **pre-training** on big clusters.\n- Check **LLama Factory** (https://github.com/hiyouga/LLaMA-Factory) for **Finetuning**.","metadata":{"id":"8d4gN4tTu47d"}}]}